\section{Query-Focused Summarization (QFS)}
Initially formulated as a document summarization task, QFS aims to generate summaries tailored to specific user queries \cite{dang-2006-duc}. Despite its potential real-world applications, QFS remains a challenging task due to the lack of datasets. In the textual domain, QFS has been explored in multi-document settings \cite{giorgi-etal-2023-open} and meeting summarization \cite{zhong-etal-2021-qmsum}. Recent datasets like QTSumm \cite{zhao2023qtsummqueryfocusedsummarizationtabular} extend QFS to a new modality, using tables as input. However, QTSumm's general-purpose nature limits its applicability to product reviews, which require nuanced reasoning over attributes and user-specific contexts. Additionally, its queries are often disconnected from real-world e-commerce scenarios. In contrast, our proposed dataset, \textbf{eC-Tab2Text}, bridges this gap by providing attribute-specific and query-driven summaries tailored to e-commerce product tables.
\section{Pretrained Models and Their Applications}
The pre-trained language models have progressed considerably due to various advances in utilizing vast amounts of data and effective training techniques in numerous natural language processing (NLP) tasks. In recent years, models such as BERT, GPT, and their derivatives, have changed the paradigm by providing highly useful general models that can be slightly adjusted to fit a new task with little training data \citep{chen-etal-2022-bert2bert}. Reuse of pre-trained models as seen in Bert2BERT where function-preserving initialization and advanced knowledge initialization increases the efficiency of pre-training large models hence reoducing the training cost and carbon footprint of training large models from scratch \citep{chen-etal-2022-bert2bert}.

\subsection{Applications in Specialized Fields}
The use of pre-trained models in fields such as clinical information extraction has shown to be effective and practical. For example, large language models, like GPT-3, have been employed to interpret intricate medical terminologies and acronyms within e-health records, thus enhancing the retrieval of pertinent medical information with minimal manual annotation \citep{agrawal2022large}. The same applies to e-commerce, where pre-trained models like GPT-4 and LLama2 have been used to pull out object features from unstructured text for efficient product searches and comparisons \citep{brinkmann2024product}. 

\subsection{Advancements in Structured Data Models}
Today, a multitude of datasets could be pulled through structured data extraction thanks to what pre-trained language models offer to e-commerce businesses. Approaches based on BERT are often data-hungry in an absolute sense and suffer from the inability to reasonably generalize to unseen attribute values or other related tasks \citep{brinkmann2024product}. In sharp contrast, modern LLMs such as GPT-4 or LLama2 exhibit deep zero-shot and few-shot ability making them very effective for attribute extraction with very little training \citep{brinkmann2024product}. Furthermore, the creation of synthetic data has also been embedded to traditional structured data models where sparse data has been addressed, which increased the performance of such models by augmenting the training datasets with examples that are both plentiful and plausible \citep{skondras2023generating}.

\subsection{Sequence-to-Sequence Architectures}
Research has shown that integrating pre-trained language model representations into sequence-to-sequence architectures can yield substantial gains in tasks like neural machine translation and abstractive summarization. For example, incorporating pre-trained embeddings into the encoder network of transformer models has significantly enhanced translation accuracy, particularly in low-resource settings, demonstrating improvements in BLEU scores and overall model performance \citep{edunov-etal-2019-pre}.

\subsection{E-commerce Systems and Personalized Solutions}
E-Commerce solutions increasingly utilize standard models and platforms such as E-BERT, infusing specialized knowledge of domain in improving recommendation, aspect extraction, and classification of products \citep{zhang2021ebert}. Optimized products like LLama2 have been shown to work and produce better results with the description of created products, thanks to the metrics like NDCG, click-through rates and studies of users \citep{zhou2023leveraging}. Moreover, when collaborative filtering is combined with LLMs recommendation systems have improved in the sense that they are now capable of providing relevant and accurate recommendations to the users \citep{xu2024emerging}.

\section{Table-to-Text Generation} 
Table-to-text generation has advanced through datasets tailored to diverse domains and applications, as summarized in Table \ref{tab:datasets}, adapted from \citep{zhao2023qtsummqueryfocusedsummarizationtabular}. Early efforts, such as WikiTableT \cite{chen2021wikitabletlargescaledatatotextdataset}, focused on generating natural language descriptions from Wikipedia tables, while TabFact \cite{2019TabFactA} introduced fact-checking capabilities and ROTOWIRE \cite{wiseman2017challengesdatatodocumentgeneration} generated detailed sports summaries. However, these datasets are limited in their relevance to product-specific domains. Later datasets like LogicNLG \cite{chen2020logicalnaturallanguagegeneration} emphasized logical inference and reasoning, and ToTTo \cite{parikh2020tottocontrolledtabletotextgeneration} supported controlled text generation by focusing on specific table regions. HiTab \cite{cheng-etal-2022-hitab} extended these capabilities with hierarchical table structures and reasoning operators. Despite these advancements, none of these datasets provide the contextual and attribute-specific depth necessary for e-commerce applications, where generating meaningful descriptions requires reasoning across heterogeneous attributes, such as linking battery capacity to battery life or associating display size with user experience.

\begin{table*}[ht]
    \footnotesize
    \centering
    \caption{Comparison between eC-Tab2Text and existing table-to-text generation datasets. \small{Adapted from \citep{zhao2023qtsummqueryfocusedsummarizationtabular}}}
    \renewcommand{\arraystretch}{1.1} % Adjusts the row spacing
    \resizebox{\textwidth}{!} 
    { 
    \begin{tblr}{hline{1,2,Z} = 0.8pt, hline{3-Y} = 0.2pt,
                 colspec = {Q[l,m, 13em] Q[l,m, 6em] Q[c,m, 8em] Q[c,m, 5em] Q[l,m, 14em]},
                 colsep  = 4pt,
                 row{1}  = {0.4cm, font=\bfseries, bg=gray!30},
                 row{2-Z} = {0.2cm},
                 }
\textbf{Dataset}       & \textbf{Table Source} & \textbf{\# Tables / Statements} & \textbf{\# Words / Statement} & \textbf{Explicit Control}\\ 
\SetCell[c=5]{c} \textit{Single-sentence Table-to-Text}\\
ToTTo \citep{parikh2020tottocontrolledtabletotextgeneration}   & Wikipedia        & 83,141 / 83,141                  & 17.4                          & Table region      \\
LOGICNLG \citep{chen2020logicalnaturallanguagegeneration} & Wikipedia        & 7,392 / 36,960                  & 14.2                          & Table regions      \\ 
HiTab \citep{cheng-etal-2022-hitab}   & Statistics web   & 3,597 / 10,672                  & 16.4                          & Table regions \& reasoning operator \\ 
\SetCell[c=5]{c} \textit{Generic Table Summarization}\\
ROTOWIRE \citep{wiseman2017challengesdatatodocumentgeneration} & NBA games      & 4,953 / 4,953                   & 337.1                         & \textbf{\textit{X}}                   \\
SciGen \citep{moosavi2021scigen} & Sci-Paper      & 1,338 / 1,338                   & 116.0                         & \textbf{\textit{X}}                   \\
NumericNLG \citep{suadaa-etal-2021-towards} & Sci-Paper   & 1,355 / 1,355                   & 94.2                          & \textbf{\textit{X}}                    \\
\SetCell[c=5]{c} \textit{Table Question Answering}\\
FeTaQA \citep{nan2021fetaqafreeformtablequestion}     & Wikipedia      & 10,330 / 10,330                 & 18.9                          & Queries rewritten from ToTTo \\
\SetCell[c=5]{c} \textit{Query-Focused Table Summarization}\\
QTSumm \citep{zhao2023qtsummqueryfocusedsummarizationtabular}                        & Wikipedia      & 2,934 / 7,111                   & 68.0                          & Queries from real-world scenarios\\ 
\textbf{eC-Tab2Text} (\textit{ours})                           & e-Commerce products      & 1,452 / 3354                   & 56.61                          & Queries from e-commerce products\\
    \end{tblr}
    }
\label{tab:datasets}
\end{table*}

\subsection{Advancements Through Synthetic Data Generation}
The advancements in synthetic data generation methods have helped alleviate the problem of constrained and underrepresentation of training data in structured datasets. To illustrate, synthetic data created by LLMs such as ChatGPT has been employed in supplementing internationally real-world datasets thus enriching resume classification models leading to improved application-specific model accuracy and robustness. \citep{skondras2023generating}.

\section{Evaluation Metrics for LLMs}

Evaluating the performance of large language models requires comprehensive metrics that reflect their capabilities across different dimensions. Traditional metrics like BLEU and ROUGE assess the quality of text generation by comparing outputs to reference texts \citep{zhang2022opt}. However, newer methods have introduced specialized metrics for diverse tasks.

\subsection{Faithfulness and Correctness}
Faithfulness measures the factual accuracy of generated content by ensuring that outputs are grounded in input data \citep{madsen-etal-2022-evaluating}. Correctness focuses on syntactic and grammatical quality, ensuring coherence and linguistic accuracy \citep{yao2023predictinggeneralizationperformancecorrectness}. Advanced evaluators like G-Eval and Prometheus provide automated scoring for these metrics, enhancing large-scale evaluation processes \citep{kim2024prometheus2opensource}.
