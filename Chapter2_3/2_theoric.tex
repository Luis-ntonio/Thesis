\section{E-commerce Product-related Databases}
Large amount of data in the field of e-commerce is procured and recorded over the years. Data regarding sales and distribution of products, reviews by users, transactions amongst other activities are integrated into databases which will have to start learning how to integrate the bulk of information \cite{Muntjir2016}. On the same line, attempts have been made through various researches to find out how to work with this data, and such include the Hadoop or MPP distributed databases \citep{5496972}. These are meant for scanning customer reviews and buying patterns, which enable businesses to make appropriate choices of the products and enhance customer experience when buying products \citep{Liang_2020}.
\\

Also in line with the progress of data management tools and frameworks, inclusion of this methodologies have emerged in the frameworks. These frameworks are adding to e-commerce platform productivity. Productpedia\footnote{\url{https://www.theproductfolks.com/productpedia-product-management-glossary}} is an example that allows one seller to setup a unified product catalog and thus making it easier to share the product information and synchronize data across the platforms \citep{10.1007/978-3-319-20895-4_34}. Other tools also provide alternatives to deal with the bulk of data and case in point is TrendSpotter, which on a real-time basis analyzes customer behavior and suggests things that people would likely want to buy where a trend has been made \citep{10.1145/3583780.3615503}. This is a significant advancement for businesses trying to keep up with the ever-changing market.


\section{Large Language Models (LLMs)} 
Large Language models are the next step of NLP architectures. These models can have millions or even billions of tokens \citep{zhao2023survey} that are trained on huge amounts of text. This allows them to handle tasks like translation, summarization, and sentiment analysis with high-confidence accuracy. LLMs are very flexible and can be used in many areas, such as improving recommendation systems, robotics, and telecommunications \citep{10305960, fan2023fatellm}.
\\

LLMs are so powerful because their ability to learn from minimal data. They can tackle tasks they have never explicitly been trained onâ€”a capability known as `zero-shot' or `few-shot' learning \citep{naveed2024comprehensive}. This flexibility makes them increasingly valuable even outside traditional NLP applications.

\section{Fine Tuning}
Fine-tuning is a technique of taking a pre-trained model and with the use of different datasets train the model increasing its knowledge and its capacity to complete new tasks \citep{Zhang2022Fine-Tuning}. This techniques allows to improve the robustness of the models in different domains and task \citep{Lalor2017Improving}. One of the advantage of fine-tuning models is that is faster to train due to the need of less data compared to training a model from scratch, making it possible to reduce computational costs and the capacity to execute some processes locally \citep{Xiao2023Offsite-Tuning:}.

\subsection{Mathematical Framework}
Fine-tuning continue increasing the knowledge the model already learned during its initial training on a large dataset. In simple terms, this process involves adjusting the model's parameters ($\theta$) to improve its performance on a specific task. The model starts with what it learned from the large dataset ($D$) and is then updated using a smaller, task-specific dataset ($D'$). This adjustment is guided by optimizing a loss function ($L$), which measures how well the model is doing \citep{Liu2023Improving}. The objective can be expressed as:

\[
\min_{\theta} L_{D'}(\theta)
\]
where $L_{D'}$ represents the loss on the fine-tuning dataset. Gradient-based methods are used to adjust the pre-trained weights minimally but effectively to improve performance on the new task \citep{Lalor2017Improving}.

\subsection{Operational Fine-Tunings}
Fine-tuning tries to making specific adjustments to the model so it can handle a new task better. It finds to add knowledge and rules related to the specific domain. The key is to make these changes without disrupting what the model already knows, so it stays stable and works consistently \citep{Catani2020A}.

\subsection{Sample Complexity and Generalization}
Fine-tuning depends on how similar the pre-training task is to the new one to achieve a good performance in the new task. Fine-tuning can significantly reduce the number of examples needed to train a model (called sample complexity), this is because the general data features the pre-trained model already knows for different task. Fine-tuning simply tweaks these features to suit the new task, often achieving good accuracy with fewer examples. This idea can be better understood by looking at how the model's ability to generalize improves after fine-tuning \citep{Shachaf2021A}.

\subsection{Gradient-Based Fine-Tuning}
Fine-tuning often involves gradient-based optimization techniques. Stochastic Gradient Descent (SGD) in mostly cases is used to iteratively adjust the weights. The process can be sensitive to the initial learning rate and other hyperparameters \citep{Vrbancic2020Transfer}. However, for LLMs fine-tuning optimizers like AdamW \citep{loshchilov2019decoupledweightdecayregularization} are often preferred due to their efficiency and stability.

\subsection{Computational Efficiency}
In computational focus, apply fine-tuning methods are efficient compared to training a model from scratch. By starting with a pre-trained model, the number of training epochs and the amount of data required are significantly reduced. This reduce of amount of data and number of training epochs leads to less computational requirements that, in some cases, permit to execute and train models locally \citep{Shi2023Towards}. Fine-tuning allows for the practical deployment of advanced models in resource-constrained environments by focusing computational resources on the most impactful aspects of training \citep{Xiao2023Offsite-Tuning:}.

\section{JSON-Tuning}

JSON-Tuning is an approach that taking advantage of JSON (JavaScript Object Notation) data structure to training LLMs with a more comprehensive and consistent data. This method improves accuracy and efficiency which agilize how data is fed into the model and reduces the workload during fine-tuning \citep{zheng2024llamafactory}.

One of the key benefits of JSON-Tuning is its ability to reduce redundancy and simplify data management. This allows the models to reduce time in inference and training having more consistency in the contexts they are learining \citep{gao2024jsontuning}. 

\section{Evaluation Metrics} \label{sec:evaluation-metrics}
\subsection{BLEU (Bilingual Evaluation Understudy)}

Measures n-gram overlap between machine-generated and reference text \citep{Reiter2018A}. Mathematically, the BLEU score is calculated using the formula:
\[
\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)
\]
where:
\begin{itemize}
    \item \( BP \) is the brevity penalty to penalize short translations.
    \item \( w_n \) is the weight for n-gram precision.
    \item \( p_n \) is the precision for n-grams of length \( n \).
\end{itemize}

Brevity penalty \( BP \) is defined as:
\[
BP = 
\begin{cases} 
1 & \text{if } c > r \\
e^{(1-\frac{r}{c})} & \text{if } c \leq r 
\end{cases}
\]
where \( c \) is the length of the candidate translation and \( r \) is the length of the reference translation \citep{Reiter2018A}.

\subsection{ROUGE (Recall-Oriented Understudy for Gisting Evaluation)} 

Focuses on recall, measuring the overlap of reference text in generated output \citep{Ng2015Better}.

\begin{enumerate}
    \item \textbf{ROUGE-N \citep{Maples2017TheR}}: Measures the n-gram recall between the candidate and reference summaries.
    \[
    \text{ROUGE-N} = \frac{\sum_{S \in \text{RefSummaries}} \sum_{gram_n \in S} \text{Count}_{match}(gram_n)}{\sum_{S \in \text{RefSummaries}} \sum_{gram_n \in S} \text{Count}(gram_n)}
    \]
    where \( gram_n \) is any n-gram, and \( \text{Count}_{match}(gram_n) \) is the maximum number of n-grams co-occurring in a candidate and reference summary.

    \item \textbf{ROUGE-L \citep{lin-2004-rouge}}: Measures the longest common subsequence (LCS) based statistics, capturing sentence-level structure similarity.
    \[
    \text{ROUGE-L} = \frac{LCS(C, R)}{\text{length}(R)}
    \]
    where \( LCS(C, R) \) is the length of the longest common subsequence between candidate \( C \) and reference \( R \) \citep{Ng2015Better}.

    \item \textbf{ROUGE-1 and ROUGE-2}: Specifically measure the overlap of unigrams and bigrams, respectively, between the candidate and reference summaries \citep{Ganesan2015ROUGE}.
\end{enumerate}

\subsection{METEOR (Metric for Evaluation of Translation with Explicit ORdering)}

Incorporates synonyms and paraphrases for evaluating translations \citep{Agarwal2008Meteor}. The final score is a harmonic mean of unigram precision and recall, favoring recall:
\[
\text{METEOR \citep{lavie-etal-2004-significance}} = \frac{10 \cdot P \cdot R}{9 \cdot P + R}
\]
where:
\begin{itemize}
    \item \( P \) is the precision of unigrams.
    \item \( R \) is the recall of unigrams.
\end{itemize}

This metric also incorporates a penalty function for longer alignment chunks to address issues of word ordering \citep{Agarwal2008Meteor}.

\subsection{BERTScore}

Leverages contextual embeddings from pre-trained transformer models to measure semantic similarity between generated and reference texts. Unlike n-gram-based metrics, BERTScore captures meaning and context, offering a robust evaluation for text generation tasks \cite{zhang2020bertscoreevaluatingtextgeneration}.

The mathematical formulation is the following:
\[
F_{\text{BERT \citep{zhang2020bertscoreevaluatingtextgeneration}}} = 2 \cdot \frac{P_{\text{BERT}} \cdot R_{\text{BERT}}}{P_{\text{BERT}} + R_{\text{BERT}}}.
\]

According with the Hugginface space \footnote{https://huggingface.co/spaces/evaluate-metric/bertscore} and \citep{zhang2020bertscoreevaluatingtextgeneration}, BERTScore can produce three different metrics:
\begin{itemize}
    \item \textbf{Precision}: Focuse on the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive \footnote{\url{https://huggingface.co/spaces/evaluate-metric/precision}}.
    \item \textbf{Recall}: The fraction of the positive examples that were correctly labeled by the model as positive \footnote{\url{https://huggingface.co/spaces/evaluate-metric/recall}}.
    \item \textbf{F1-score}: The harmonic mean of the precision and recall \footnote{\url{https://huggingface.co/spaces/evaluate-metric/f1}}.
\end{itemize}

\section{Faithfulness, Fluency and Correctness in LLMs}

Faithfulness, fluency and correctness are metrics usually used in the evaluation of large language models (LLM) systems as model-based metrics. Using these metrics it is possible to evaluate the performance of the output generated capturing the context of all the text instead of the words-metrics \citep{lyu2024faithfulmodelexplanationnlp}.


\subsection{Faithfulness}
Faithfulness evaluate the model ability of creating outputs using factual infromation given by the context avoiding generating information that its origin is unknown  \citep{jacovi-goldberg-2020-towards}.

Faithfulness can be measured in a few ways: 
\begin{itemize}
    \item \textbf{Reference-based evaluation}: This compares the model's output to a reference or correct answer. If the output matches the source text, it is considered faithful \citep{parcalabescu2024measuringfaithfulnessselfconsistencynatural}.  
    \item \textbf{Model-based evaluation}: Specialized models like Prometheus \citep{kim2024prometheus2opensource} or G-eval \cite{liu2023gevalnlgevaluationusing} check if the output is consistent with the input and spot any deviations \citep{gat2023faithfulexplanationsblackboxnlp}.  
    \item \textbf{Human evaluation}: People manually review the output to see if it accurately represents the input. This method often involves subjective scoring of factual accuracy \citep{jacovi-goldberg-2020-towards}.  
\end{itemize}


\subsection{Correctness}
Correctness metric especially evaluate the structure of the output, if the syntaxis is correct, follows grammar rules and mantaining some coherence in the text \citep{varshney-etal-2022-towards}.

Correctness can be evaluated by: 
\begin{itemize} 
    \item Linguistic accuracy: Focused on the gramar and context of the text \citep{varshney-etal-2022-towards}. 
    \item Semantic accuracy: Evaluate if the output is meaningful and coherent within the context of the task \citep{steen2023littlepushnlimodels}. 
    \item Automatic metrics: Metrics such as BLEU, ROUGE, or METEOR can be used too to measure how closely the generated output matches the reference text in terms of word overlap, sequence structure, and linguistic integrity \citep{gat2023faithfulexplanationsblackboxnlp}. 
    \item Model-based evaluation: Correctness can be evaluated with Prometheus or G-eval too\citep{kim2024prometheus2opensource}.\end{itemize}

\subsection{Fluency}
Evaluates the readability and linguistic quality of the text, ensuring it adheres to natural language norms \cite{suadaa-etal-2021-towards, Lee2023ASO}.

Fluency can be evaluated through various approaches which includes the following:
\begin{itemize}
    \item \textbf{Linguistic coherence}: Evaluating the generated text's logical flow and sentence connections to make sure the final product is coherent and makes sense in its context \citep{gat2023faithfulexplanationsblackboxnlp}.
    \item \textbf{Grammatical accuracy}: Examine the grammatical mistakes that can be causing the reading to be less fluent. \citep{varshney-etal-2022-towards}.
    \item \textbf{Stylistic consistency}: Pay attention to the outputs' vocabulary, formality, and tone, and assess them using the task's intended style. \citep{yao2023predictinggeneralizationperformancecorrectness}.
    \item \textbf{Human evaluation}: Based on many attributes that the advisers provide, a human can rate the text's fluency, frequently offering insights that supplement automated measures. \citep{jacovi-goldberg-2020-towards}.
    \item \textbf{Model-based evaluation}: Using models like Prometheus to assess linguistic quality and stylistic alignment \citep{kim2024prometheus2opensource}.
\end{itemize}

Fluency is particularly relevant in applications requiring user interaction, if the fluency is poor it can lead to misunderstandings, reduced trust, and disengagement of the users. Fluency ensures that the output is not only accurate but also appealing and easy to comprehend \citep{jacovi-goldberg-2020-towards}.

\section{Cross-Validation Evaluation}

To check how well our models can generalize and handle new data, we use a cross-validation approach. Cross-validation is a widely used technique that splits the data into multiple subsets (folds) and alternates between training and testing on these folds \citep{jiang2017markov, carmack2012generalised, Bergmeir2012On}. This helps measure the model's performance on unseen data. To increase the valuability of the research, we implemented cross-validation that tests model robustness on different dataset \citep{Barratt2018OptimizingFG}.

\subsection{Cross-Validation with Alternate Datasets}

Two distinct datasets, $A$ and $B$, can be used to test the robustness of a model or dataset in order to fine-tune an LLM. To ensure that a model performs well across several data types, it is intended to be trained on one dataset and tested on another. In other words:

\begin{itemize}
    \item Train a model, $M_A$, on dataset $A$ and test it on dataset $B$.
    \item Train another model, $M_B$, on dataset $B$ and test it on dataset $A$.
\end{itemize}

If the models perform well on the alternate datasets, it means they have learned meaningful patterns rather than just memorizing the training data.

\subsection{Mathematical Formulation}

To explain Mathematically how cross-validation will be applied on the research, let $\mathcal{D}_A = \{(x_i^A, y_i^A)\}_{i=1}^{n_A}$ and $\mathcal{D}_B = \{(x_i^B, y_i^B)\}_{i=1}^{n_B}$ represent two datasets with $n_A$ and $n_B$ samples. The cross-validation process involves:

\begin{enumerate}
    \item \textbf{Training Models}: 
   \begin{align}
       M_A &= \text{train}(\mathcal{D}_A), \\
       M_B &= \text{train}(\mathcal{D}_B).
   \end{align}

    \item \textbf{Cross-Dataset Testing}: 
   \begin{itemize}
    \item Test $M_A$ on $\mathcal{D}_B$ to calculate the error $E(M_A, \mathcal{D}_B)$.
    \item Test $M_B$ on $\mathcal{D}_A$ to calculate the error $E(M_B, \mathcal{D}_A)$.
   \end{itemize} 

    \item \textbf{Performance Metrics}: To evaluate the performance of the models we applied metrics based on text and  model as explained on section \ref{sec:evaluation-metrics}:
\end{enumerate}
Comparing the metrics of in-dataset and cross-dataset testing helps us understand how well the models can generalize and adapt to new data.

\section{Summary}

This chapter explores how machine learning and advanced data systems are transforming e-commerce. From improving search results using big data to predicting trends with machine learning, these technologies enhance the shopping experience.
\\

It also remarks the role of large language models in NLP, showing how fine-tuning and techniques like JSON-Tuning make them adaptable to specific domains. Finally, it focus in the importance of evaluation metrics and validation techniques for ensuring models are accurate and reliable in real-world scenarios with word-based metrics and model-based ones.
