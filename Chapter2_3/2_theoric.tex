\section{E-commerce Product-related Databases}
E-commerce is a vast industry that continues to grow rapidly, managing large amount of product data, users, reviews and transactions. To mantain the performance and the user experience, platforms integrated databases with advanced queries and big data techniques. Studies have shown that incorporating these types of queries into e-commerce systems can streamline the search process, making it more user-friendly overall \citep{Muntjir2016}. Big data tools, like Hadoop \citep{5496972} or MPP distributed databases, are also being used to analyze customer reviews and buying habits. This helps businesses optimize product selection and create a better shopping experience for customers \citep{Liang_2020}.
\\

Due the problem of handling complex data formats, different frameworks has emerged. These frameworks are helping e-commerce platforms run more efficiently. Cloud-based systems like \footnote{\url{https://www.theproductfolks.com/productpedia-product-management-glossary}} allow sellers to maintain a centralized product catalog, making it easier to sync data across platforms and share rich product information \citep{10.1007/978-3-319-20895-4_34}. Machine learning tools also presents alternatives to handle complex data, like TrendSpotter \citep{10.1145/3583780.3615503}, which can predict trending products by analyzing customer behavior in real time. This is a significant advancement for businesses trying to keep up with the ever-changing market.


\section{Large Language Models (LLMs)} %falta hablar desde el punto de vista computacional
Large language models (LLMs) are a big step forward in natural language processing (NLP). These models, which have millions or even billions of settings \citep{zhao2023survey}, are trained on huge amounts of text. This allows them to handle tasks like translation, summarization, and sentiment analysis with impressive accuracy. LLMs are very flexible and can be used in many areas, such as improving recommendation systems, robotics, and telecommunications \citep{10305960, fan2023fatellm}.
\\

LLMs are so powerful because their ability to learn from minimal data. They can tackle tasks they have never explicitly been trained onâ€”a capability known as `zero-shot' or `few-shot' learning \citep{naveed2024comprehensive}. This flexibility makes them increasingly valuable even outside traditional NLP applications.

\section{Fine Tuning} %especifcar como se realiza el fine tuning de forma teorica
Fine-tuning is a technique of taking a pre-trained model and tailoring it for a specific task \citep{Zhang2022Fine-Tuning}. A general-purpose language model can be fine-tuned on a smaller, domain-specific dataset to analyze e-commerce reviews more effectively.

\subsection{The Basics}
The process adjusts the model's parameters to minimize a loss function \(L\) on a smaller dataset \(D'\), leveraging the knowledge the model has already learned \citep{Lalor2017Improving}. The key is to make incremental changes to the model's weights without erasing its general-purpose capabilities.

\subsection{Practical Fine-Tuning}
Fine-tuning often employs gradient-based methods like Stochastic Gradient Descent (SGD). However, the low variability of some datasets may lead in overfitting problems which can degrade the model's performance on general tasks \citep{Catani2020A}.

\subsection{Why It's Efficient}
The main advantage of fine-tuning models is that is faster and requires less data compared to training a model from scratch, making it ideal for scenarios with limited computational resources \citep{Xiao2023Offsite-Tuning:}.

\subsection{Mathematical Framework}
Fine-tuning continue increasing the knowledge the model already learned during its initial training on a large dataset. In simple terms, this process involves adjusting the model's parameters ($\theta$) to improve its performance on a specific task. The model starts with what it learned from the large dataset ($D$) and is then updated using a smaller, task-specific dataset ($D'$). This adjustment is guided by optimizing a loss function ($L$), which measures how well the model is doing \citep{Liu2023Improving}. The objective can be expressed as:

\[
\min_{\theta} L_{D'}(\theta)
\]
where $L_{D'}$ represents the loss on the fine-tuning dataset. Gradient-based methods are used to adjust the pre-trained weights minimally but effectively to improve performance on the new task \citep{Lalor2017Improving}.

\subsection{Operational Fine-Tunings}
Fine-tuning tries to making specific adjustments to the model so it can handle a new task better. It finds to add knowledge and rules related to the specific domain. The key is to make these changes without disrupting what the model already knows, so it stays stable and works consistently \citep{Catani2020A}.

\subsection{Sample Complexity and Generalization}
Fine-tuning depends on how similar the pre-training task is to the new one to achieve a good performance in the new task. Fine-tuning can significantly reduce the number of examples needed to train a model (called sample complexity), this is because the general data features the pre-trained model already knows for different task. Fine-tuning simply tweaks these features to suit the new task, often achieving good accuracy with fewer examples. This idea can be better understood by looking at how the model's ability to generalize improves after fine-tuning \citep{Shachaf2021A}.

\subsection{Gradient-Based Fine-Tuning}
Fine-tuning often involves gradient-based optimization techniques. Stochastic Gradient Descent (SGD) in mostly cases is used to iteratively adjust the weights. The process can be sensitive to the initial learning rate and other hyperparameters \citep{Vrbancic2020Transfer}. However, for LLMs fine-tuning optimizers like AdamW \citep{loshchilov2019decoupledweightdecayregularization} are often preferred due to their efficiency and stability.

\subsection{Computational Efficiency}
In computational focus, apply fine-tuning methods are efficient compared to training a model from scratch. By starting with a pre-trained model, the number of training epochs and the amount of data required are significantly reduced. This reduce of amount of data and number of training epochs leads to less computational requirements that, in some cases, permit to execute and train models locally \citep{Shi2023Towards}. Fine-tuning allows for the practical deployment of advanced models in resource-constrained environments by focusing computational resources on the most impactful aspects of training \citep{Xiao2023Offsite-Tuning:}.

\section{JSON-Tuning}

JSON-Tuning is an approach that taking advantage of JSON (JavaScript Object Notation) data structure to training LLMs with a more comprehensive and consistent data. This method improves accuracy and efficiency which agilize how data is fed into the model and reduces the workload during fine-tuning \citep{zheng2024llamafactory}.

One of the key benefits of JSON-Tuning is its ability to reduce redundancy and simplify data management. This allows the models to reduce time in inference and training having more consistency in the contexts they are learining \citep{gao2024jsontuning}. 

\section{Evaluation Metrics}
\subsection{BLEU (Bilingual Evaluation Understudy)}

Measures n-gram overlap between machine-generated and reference text \citep{Reiter2018A}. Mathematically, the BLEU score is calculated using the formula:
\[
\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)
\]
where:
\begin{itemize}
    \item \( BP \) is the brevity penalty to penalize short translations.
    \item \( w_n \) is the weight for n-gram precision.
    \item \( p_n \) is the precision for n-grams of length \( n \).
\end{itemize}

Brevity penalty \( BP \) is defined as:
\[
BP = 
\begin{cases} 
1 & \text{if } c > r \\
e^{(1-\frac{r}{c})} & \text{if } c \leq r 
\end{cases}
\]
where \( c \) is the length of the candidate translation and \( r \) is the length of the reference translation \citep{Reiter2018A}.

\subsection{ROUGE (Recall-Oriented Understudy for Gisting Evaluation)} 

Focuses on recall, measuring the overlap of reference text in generated output \citep{Ng2015Better}.

\begin{enumerate}
    \item \textbf{ROUGE-N \citep{Maples2017TheR}}: Measures the n-gram recall between the candidate and reference summaries.
    \[
    \text{ROUGE-N} = \frac{\sum_{S \in \text{RefSummaries}} \sum_{gram_n \in S} \text{Count}_{match}(gram_n)}{\sum_{S \in \text{RefSummaries}} \sum_{gram_n \in S} \text{Count}(gram_n)}
    \]
    where \( gram_n \) is any n-gram, and \( \text{Count}_{match}(gram_n) \) is the maximum number of n-grams co-occurring in a candidate and reference summary.

    \item \textbf{ROUGE-L \citep{lin-2004-rouge}}: Measures the longest common subsequence (LCS) based statistics, capturing sentence-level structure similarity.
    \[
    \text{ROUGE-L} = \frac{LCS(C, R)}{\text{length}(R)}
    \]
    where \( LCS(C, R) \) is the length of the longest common subsequence between candidate \( C \) and reference \( R \) \citep{Ng2015Better}.

    \item \textbf{ROUGE-1 and ROUGE-2}: Specifically measure the overlap of unigrams and bigrams, respectively, between the candidate and reference summaries \citep{Ganesan2015ROUGE}.
\end{enumerate}

\subsection{METEOR (Metric for Evaluation of Translation with Explicit ORdering)}

Incorporates synonyms and paraphrases for evaluating translations \citep{Agarwal2008Meteor}. The final score is a harmonic mean of unigram precision and recall, favoring recall:
\[
\text{METEOR \citep{lavie-etal-2004-significance}} = \frac{10 \cdot P \cdot R}{9 \cdot P + R}
\]
where:
\begin{itemize}
    \item \( P \) is the precision of unigrams.
    \item \( R \) is the recall of unigrams.
\end{itemize}

This metric also incorporates a penalty function for longer alignment chunks to address issues of word ordering \citep{Agarwal2008Meteor}.

\subsection{BERTScore}

Uses contextual embeddings to assess semantic similarity between generated and reference texts \citep{zhang2020bertscoreevaluatingtextgeneration}.

The mathematical formulation is the following:
\[
F_{\text{BERT \citep{zhang2020bertscoreevaluatingtextgeneration}}} = 2 \cdot \frac{P_{\text{BERT}} \cdot R_{\text{BERT}}}{P_{\text{BERT}} + R_{\text{BERT}}}.
\]

According with the Hugginface space \footnote{https://huggingface.co/spaces/evaluate-metric/bertscore} and \citep{zhang2020bertscoreevaluatingtextgeneration}, BERTScore can produce three different metrics:
\begin{itemize}
    \item \textbf{Precision}: The fraction of correctly labeled positive examples out of all of the examples that were labeled as positive.
    \item \textbf{Recall}: The fraction of the positive examples that were correctly labeled by the model as positive.
    \item \textbf{F1-score}: The harmonic mean of the precision and recall.
\end{itemize}

\section{Faithfulness, Fluency and Correctness in LLMs}

Faithfulness, fluency and correctness are metrics usually used in the evaluation of large language models (LLM) systems as model-based metrics. Using these metrics it is possible to evaluate the performance of the output generated capturing the context of all the text instead of the words-metrics \citep{lyu2024faithfulmodelexplanationnlp}.


\subsection{Faithfulness}
Faithfulness evaluate the model ability of creating outputs using factual infromation given by the context avoiding generating information that its origin is unknown  \citep{jacovi-goldberg-2020-towards}.

Faithfulness can be measured in a few ways: 
\begin{itemize}
    \item \textbf{Reference-based evaluation}: This compares the model's output to a reference or correct answer. If the output matches the source text, it is considered faithful \citep{parcalabescu2024measuringfaithfulnessselfconsistencynatural}.  
    \item \textbf{Model-based evaluation}: Specialized models like Prometheus \citep{kim2024prometheus2opensource} or G-eval \cite{liu2023gevalnlgevaluationusing} check if the output is consistent with the input and spot any deviations \citep{gat2023faithfulexplanationsblackboxnlp}.  
    \item \textbf{Human evaluation}: People manually review the output to see if it accurately represents the input. This method often involves subjective scoring of factual accuracy \citep{jacovi-goldberg-2020-towards}.  
\end{itemize}


\subsection{Correctness}
Correctness metric especially evaluate the structure of the output, if the syntaxis is correct, follows grammar rules and mantaining some coherence in the text \citep{varshney-etal-2022-towards}.

Correctness can be evaluated by: 
\begin{itemize} 
    \item Linguistic accuracy: Focused on the gramar and context of the text \citep{varshney-etal-2022-towards}. 
    \item Semantic accuracy: Evaluate if the output is meaningful and coherent within the context of the task \citep{steen2023littlepushnlimodels}. 
    \item Automatic metrics: Metrics such as BLEU, ROUGE, or METEOR can be used too to measure how closely the generated output matches the reference text in terms of word overlap, sequence structure, and linguistic integrity \citep{gat2023faithfulexplanationsblackboxnlp}. 
    \item Model-based evaluation: Correctness can be evaluated with Prometheus or G-eval too\citep{kim2024prometheus2opensource}.\end{itemize}

\subsection{Fluency}
Fluency evaluate how nmatural the text generated is, focuse on the smoothness, easy-reading, and how much the text is human-like. Evaluate the sentences too check the logical flow of the text. Fluency is a critical metric for evaluating LLM outputs in tasks such as conversational agents, creative writing, and summarization, where readability and user engagement are paramount \citep{yin2022sensitivitystabilitymodelinterpretations}.

Fluency can be evaluated through various approaches:
\begin{itemize}
    \item \textbf{Linguistic coherence}: Assessing the logical progression and connectivity of sentences in the generated text, ensuring that the output is cohesive and makes sense within the context \citep{gat2023faithfulexplanationsblackboxnlp}.
    \item \textbf{Grammatical accuracy}: Evaluate the gramatical errors that may involved in reducing the fluemcy of the reading \citep{varshney-etal-2022-towards}.
    \item \textbf{Stylistic consistency}: Focuse on the tone, formality, and vocabulary of the outputs are consistent with the intended style of the task \citep{yao2023predictinggeneralizationperformancecorrectness}.
    \item \textbf{Human evaluation}: It is possible to ask human to score the text's fluency based on readability and naturalness, often providing insights that complement automatic metrics \citep{jacovi-goldberg-2020-towards}.
    \item \textbf{Model-based evaluation}: Employing models or tools like Prometheus to assess linguistic quality and stylistic alignment \citep{kim2024prometheus2opensource}.
\end{itemize}

Fluency is particularly relevant in applications requiring user interaction, if the fluency is poor it can lead to misunderstandings, reduced trust, and disengagement of the users. Fluency ensures that the output is not only accurate but also appealing and easy to comprehend \citep{jacovi-goldberg-2020-towards}.

\section{Cross-Validation Evaluation}

To check how well our models can generalize and handle new data, we use a cross-validation approach. Cross-validation is a widely used technique that splits the data into multiple subsets (folds) and alternates between training and testing on these folds \citep{jiang2017markov, carmack2012generalised, Bergmeir2012On}. This helps measure the model's performance on unseen data. In this study, we take it a step further by using a variant of cross-validation that tests model robustness on completely different datasets \citep{Barratt2018OptimizingFG}.

\subsection{Cross-Validation with Alternate Datasets}

To ensure the robustness of a model or dataset to fine-tune an LLM, we perform cross-validation using two separate datasets, $A$ and $B$. The idea is to train a model on one dataset and test it on the other, making sure the model works well across different types of data. Specifically:

\begin{itemize}
    \item Train a model, $M_A$, on dataset $A$ and test it on dataset $B$.
    \item Train another model, $M_B$, on dataset $B$ and test it on dataset $A$.
\end{itemize}

If the models perform well on the alternate datasets, it means they have learned meaningful patterns rather than just memorizing the training data.

\subsection{Mathematical Formulation}

Let $\mathcal{D}_A = \{(x_i^A, y_i^A)\}_{i=1}^{n_A}$ and $\mathcal{D}_B = \{(x_i^B, y_i^B)\}_{i=1}^{n_B}$ represent two datasets with $n_A$ and $n_B$ samples. The cross-validation process involves:

1. \textbf{Training Models}: 
   \begin{align}
       M_A &= \text{train}(\mathcal{D}_A), \\
       M_B &= \text{train}(\mathcal{D}_B).
   \end{align}

2. \textbf{Cross-Dataset Testing}: 
   - Test $M_A$ on $\mathcal{D}_B$ to calculate the error $E(M_A, \mathcal{D}_B)$.
   - Test $M_B$ on $\mathcal{D}_A$ to calculate the error $E(M_B, \mathcal{D}_A)$.

3. \textbf{Performance Metrics}: 
   We evaluate each model's performance on the alternate dataset using metrics like accuracy, precision, recall, or mean squared error (MSE). For example, if we use MSE:
   \begin{align}
       \text{MSE}_{M_A \rightarrow B} &= \frac{1}{n_B} \sum_{i=1}^{n_B} \left(y_i^B - M_A(x_i^B)\right)^2, \\
       \text{MSE}_{M_B \rightarrow A} &= \frac{1}{n_A} \sum_{i=1}^{n_A} \left(y_i^A - M_B(x_i^A)\right)^2.
   \end{align}

By comparing errors from in-domain and out-of-domain tests (e.g., $E(M_A, \mathcal{D}_A)$ vs. $E(M_A, \mathcal{D}_B)$), we can assess if the models generalize well or are overfitting to specific data patterns.

\subsection{Discussion of Cross-Dataset Validation Results}

Analyzing the cross-dataset performance of $M_A$ and $M_B$ helps us understand how robust and adaptable these models are. If they perform consistently across datasets, it shows they can transfer what they have learned and aren not overly biased toward any specific dataset. This makes the models more reliable for broader applications.

\section{Summary}

This chapter explores how machine learning and advanced data systems are transforming e-commerce. From improving search results using big data to predicting trends with machine learning, these technologies enhance the shopping experience.
\\

It also highlights the role of large language models in NLP, showing how fine-tuning and techniques like JSON-Tuning make them adaptable to specific domains. Finally, it emphasizes the importance of evaluation metrics and validation techniques for ensuring models are accurate and reliable in real-world scenarios with word-based metrics and model-based ones.
