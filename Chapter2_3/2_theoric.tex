\section{E-commerce Product-related Databases}
In the rapidly evolving world of e-commerce, managing and utilizing product-related databases has become more advanced. Recent developments focus on integrating sophisticated database queries and big data technologies to improve the efficiency and precision of product searches. Research indicates that incorporating database queries into e-commerce platforms significantly streamlines the search process, making it more user-friendly and effective \cite{Muntjir2016}. Additionally, using big data technologies like Hadoop and MPP distributed databases enables detailed analysis of customer reviews and purchasing trends, optimizing product selection and enhancing user experience \cite{Liang_2020}.
\\\\
The advancement of database technologies has also led to the creation of new frameworks that support complex data formats and improve the efficiency of e-commerce platforms. For instance, cloud computing-based platforms such as Productpedia help create a centralized electronic product catalog, allowing seamless data synchronization and enabling merchants to define and share semantically rich product information \cite{10.1007/978-3-319-20895-4_34}. Moreover, deploying machine learning models like TrendSpotter helps e-commerce platforms predict and highlight trending products by analyzing current customer engagement data, thereby meeting the market's dynamic demands \cite{10.1145/3583780.3615503}.

\section{Large Language Models (LLMs)} %falta hablar desde el punto de vista computacional
Large language models (LLMs) represent significant progress in natural language processing (NLP), transitioning from statistical to neural models. The term "large language model" generally refers to pre-trained language models of substantial size, often containing hundreds of millions to billions of parameters \cite{zhao2023survey}.
\\\\
These models are trained on extensive text datasets using self-supervised learning techniques, enabling them to generate human-like text and perform tasks such as translation, summarization, and sentiment analysis. Due to their extensive training data and sophisticated architectures, LLMs can capture complex language patterns and demonstrate impressive zero-shot and few-shot learning capabilities \cite{naveed2024comprehensive}.
\\\\
Beyond typical NLP tasks, LLMs are utilized in various fields. They show potential in improving recommendation systems, executing complex planning, and contributing to areas like telecommunications and robotics \cite{10305960} \cite{fan2023fatellm}.

\section{Fine Tuning} %especifcar como se realiza el fine tuning de forma teorica
Fine-tuning in machine learning is a process where a pre-trained model is adapted to a new, often related task by continuing the training process on a smaller, task-specific dataset. This process is crucial for enhancing model performance and achieving better generalization on the new task.

\subsection{Mathematical Framework}
Fine-tuning leverages the pre-existing knowledge embedded in the model parameters from the initial training on a large dataset. Mathematically, this involves optimizing a loss function $L$ with respect to the model parameters $\theta$, which have been pre-trained on a large-scale dataset $D$. The fine-tuning process then adjusts these parameters using a smaller dataset $D'$ specific to the new task. The objective can be expressed as:

\[
\min_{\theta} L_{D'}(\theta)
\]
where $L_{D'}$ represents the loss on the fine-tuning dataset. This optimization typically uses gradient-based methods to adjust the pre-trained weights minimally but effectively to improve performance on the new task \cite{Lalor2017Improving}.

\subsection{Operational Fine-Tunings}
In a more abstract sense, fine-tuning can be seen as an operational fine-tuning where the changes made to the model parameters are tailored to the specifics of the new task. This concept extends beyond traditional parameter optimization, embedding domain-specific knowledge and constraints into the model adjustments. Operational fine-tunings often require ensuring that the adjustments do not lead to significant deviations from the model's prior capabilities, ensuring stability and performance consistency \cite{Catani2020A}.

\subsection{Sample Complexity and Generalization}
The effectiveness of fine-tuning is influenced by the similarity between the pre-training and fine-tuning tasks. The sample complexity, which is the number of training examples required to achieve a certain level of performance, is significantly reduced when fine-tuning is applied. This reduction occurs because the pre-trained model already captures a broad set of features relevant to many tasks. Fine-tuning adjusts these features to better fit the new task, often requiring fewer samples to achieve high accuracy. This relationship can be formalized by analyzing the changes in the generalization bounds of the model after fine-tuning \cite{Shachaf2021A}.

\subsection{Gradient-Based Fine-Tuning}
Fine-tuning often involves gradient-based optimization techniques. For deep neural networks, this means leveraging algorithms like Stochastic Gradient Descent (SGD) to iteratively adjust the weights. The process can be sensitive to the initial learning rate and other hyperparameters, which need to be carefully chosen to avoid large deviations from the pre-trained weights and ensure convergence to a new, optimal set of parameters for the fine-tuning task \cite{Vrbancic2020Transfer}.

\subsection{Computational Efficiency}
Fine-tuning is computationally efficient compared to training a model from scratch. By starting with a pre-trained model, the number of training epochs and the amount of data required are significantly reduced. This efficiency is particularly beneficial for large-scale models where the computational cost of full training is prohibitive. Fine-tuning allows for the practical deployment of advanced models in resource-constrained environments by focusing computational resources on the most impactful aspects of training \cite{Xiao2023Offsite-Tuning:}.

\section{JSON-Tuning} 
JSON-Tuning is a novel approach aimed at enhancing the performance and efficiency of Large Language Models (LLMs) by leveraging the structured data representation capabilities of JSON (JavaScript Object Notation). This method utilizes JSON's hierarchical structure to optimize the input-output processes of LLMs, leading to better parameter tuning and improved model interpretability. JSON-Tuning provides more precise control over training data, resulting in more robust and contextually accurate predictions. This approach also facilitates efficient data organization, simplifying management and utilization during the training and fine-tuning stages of LLM development \cite{zheng2024llamafactory}.
\\\\
The benefits of JSON-Tuning extend beyond performance improvements. This technique can substantially reduce the computational load typically associated with traditional fine-tuning methods. By streamlining data processing and minimizing redundancy, JSON-Tuning enables the deployment of LLMs in real-time applications where speed and accuracy are essential. Additionally, JSON's structured nature allows for seamless integration with existing data pipelines and APIs, simplifying workflows for data scientists and developers \cite{zhu2024lift}. This combination of structured data representation and advanced model tuning offers a promising avenue for future research and development in machine learning.
%falta mencionar las metricas de evaluacion

\section{Evaluation Metrics}
\subsection{BLEU (Bilingual Evaluation Understudy)}

The BLEU metric is a widely-used method for evaluating the quality of text which has been machine-translated from one language to another. BLEU measures the correspondence between a machine's output and that of a human by calculating the precision of n-grams (sequences of words) in the generated text relative to a reference translation. Mathematically, the BLEU score is calculated using the formula:
\[
\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)
\]
where:
\begin{itemize}
    \item \( BP \) is the brevity penalty to penalize short translations.
    \item \( w_n \) is the weight for n-gram precision.
    \item \( p_n \) is the precision for n-grams of length \( n \).
\end{itemize}

Brevity penalty \( BP \) is defined as:
\[
BP = 
\begin{cases} 
1 & \text{if } c > r \\
e^{(1-\frac{r}{c})} & \text{if } c \leq r 
\end{cases}
\]
where \( c \) is the length of the candidate translation and \( r \) is the length of the reference translation \cite{Reiter2018A}.

\subsection{ROUGE (Recall-Oriented Understudy for Gisting Evaluation)}

ROUGE is a set of metrics used for evaluating automatic summarization and machine translation that measures the overlap between the generated output and a reference output. Key variants include ROUGE-N, ROUGE-L, and ROUGE-W.

\begin{enumerate}
    \item \textbf{ROUGE-N}: Measures the n-gram recall between the candidate and reference summaries.
    \[
    \text{ROUGE-N} = \frac{\sum_{S \in \text{RefSummaries}} \sum_{gram_n \in S} \text{Count}_{match}(gram_n)}{\sum_{S \in \text{RefSummaries}} \sum_{gram_n \in S} \text{Count}(gram_n)}
    \]
    where \( gram_n \) is any n-gram, and \( \text{Count}_{match}(gram_n) \) is the maximum number of n-grams co-occurring in a candidate and reference summary.

    \item \textbf{ROUGE-L}: Measures the longest common subsequence (LCS) based statistics, capturing sentence-level structure similarity.
    \[
    \text{ROUGE-L} = \frac{LCS(C, R)}{\text{length}(R)}
    \]
    where \( LCS(C, R) \) is the length of the longest common subsequence between candidate \( C \) and reference \( R \) \cite{Ng2015Better}.

    \item \textbf{ROUGE-1 and ROUGE-2}: Specifically measure the overlap of unigrams and bigrams, respectively, between the candidate and reference summaries \cite{Ganesan2015ROUGE}.
\end{enumerate}

\subsection{METEOR (Metric for Evaluation of Translation with Explicit ORdering)}

METEOR evaluates translations by aligning them to human-created reference translations using various modules such as exact matching, stemming, synonymy matching, and paraphrase matching. The final score is a harmonic mean of unigram precision and recall, favoring recall:
\[
\text{METEOR} = \frac{10 \cdot P \cdot R}{R + 9 \cdot P}
\]
where:
\begin{itemize}
    \item \( P \) is the precision of unigrams.
    \item \( R \) is the recall of unigrams.
\end{itemize}

This metric also incorporates a penalty function for longer alignment chunks to address issues of word ordering \cite{Agarwal2008Meteor}.

\section{Faithfulness and Correctness in LLMs}

Faithfulness and correctness are critical metrics in the evaluation of large language models (LLM) systems, especially when these systems generate or summarize content. These two aspects are essential to ensuring the reliability and utility of LLM models, particularly for tasks that require accurate and truthful information \cite{lyu2024faithfulmodelexplanationnlp}.

\subsection{Faithfulness}

Faithfulness refers to the extent to which the output generated by an LLM model accurately reflects the information presented in the input. In other words, a faithful response is one that does not hallucinate or introduce information that was not present in the source data. Faithfulness is particularly important in tasks such as summarization, translation, or question-answering, where factual integrity is paramount \cite{madsen-etal-2022-evaluating}. A model's output should stay grounded in the input data and avoid the generation of irrelevant or misleading content \cite{yin2022sensitivitystabilitymodelinterpretations}.

Faithfulness can be evaluated through various means, including: \begin{itemize} \item Reference-based evaluation: Comparing the generated output to a reference or ground truth text. If the model's response remains true to the source text, it is considered faithful \cite{parcalabescu2024measuringfaithfulnessselfconsistencynatural}. \item Model-based evaluation: Utilizing models designed to assess factual consistency, such as Prometheus \cite{kim2024prometheus2opensource}, which can detect whether the generated output deviates from the input \cite{gat2023faithfulexplanationsblackboxnlp}. \item Human evaluation: Asking human evaluators to manually assess whether the information in the output is a faithful representation of the input, often resulting in subjective ratings of factual accuracy \cite{jacovi-goldberg-2020-towards}. \end{itemize}

\subsection{Correctness}

Correctness refers to the syntactic and grammatical quality of the generated text, ensuring that the output is well-formed, coherent, and conforms to the rules of the target language. Correctness is fundamental to producing natural and comprehensible sentences, making it vital for tasks like machine translation, text summarization, and conversational agents \cite{yao2023predictinggeneralizationperformancecorrectness}.

Correctness can be evaluated by: \begin{itemize} \item Linguistic accuracy: Ensuring that the generated text follows the proper syntactic structure and grammar rules of the language \cite{varshney-etal-2022-towards}. \item Semantic accuracy: Evaluating whether the output is meaningful and coherent within the context of the task \cite{steen2023littlepushnlimodels}. \item Automatic metrics: Utilizing metrics such as BLEU, ROUGE, or METEOR to measure how closely the generated output matches the reference text in terms of word overlap, sequence structure, and linguistic integrity \cite{gat2023faithfulexplanationsblackboxnlp}. \item Model-based evaluation: As faithfulness, correctness can be evaluated with Prometheus too\cite{kim2024prometheus2opensource}.\end{itemize}

In LLM tasks where both factual accuracy and linguistic quality are important, faithfulness and correctness complement each other, ensuring that the output is both reliable in terms of content and clear in its presentation \cite{jacovi-goldberg-2020-towards}.

\section{Cross-Validation Evaluation}

To assess the robustness and generalization ability of the models, we apply a cross-validation evaluation methodology. Cross-validation is a powerful technique commonly used to measure a model's predictive performance on unseen data by partitioning the data into multiple subsets (folds) and iteratively training and testing the model on different folds. In this study, we employ a specific variant of cross-validation designed to test the robustness of models by evaluating their performance on alternate datasets.

\subsection{Cross-Validation with Alternate Datasets}

We perform a cross-validation process using two distinct datasets, $A$ and $B$, to verify the robustness of our trained models. This approach involves training a model on one dataset and testing it on the other, ensuring that the model generalizes well across different data distributions. Specifically:

\begin{itemize}
    \item Train the model, denoted by $M_A$, on dataset $A$ and evaluate it on dataset $B$.
    \item Train another model, denoted by $M_B$, on dataset $B$ and evaluate it on dataset $A$.
\end{itemize}

This process, often referred to as cross-dataset validation, provides insight into the models' robustness and generalizability, as a high performance on the alternate dataset implies that the model has learned meaningful patterns rather than overfitting to specific characteristics of its training data.

\subsection{Mathematical Formulation}

Let $\mathcal{D}_A = \{(x_i^A, y_i^A)\}_{i=1}^{n_A}$ and $\mathcal{D}_B = \{(x_i^B, y_i^B)\}_{i=1}^{n_B}$ represent the two datasets with $n_A$ and $n_B$ samples, respectively. The cross-validation evaluation involves the following steps:

1. \b{Training Models}: 
   \begin{align}
       M_A &= \text{train}(\mathcal{D}_A), \\
       M_B &= \text{train}(\mathcal{D}_B).
   \end{align}

2. \b{Cross-Dataset Testing}: 
   - Evaluate $M_A$ on $\mathcal{D}_B$, resulting in an error metric $E(M_A, \mathcal{D}_B)$.
   - Evaluate $M_B$ on $\mathcal{D}_A$, resulting in an error metric $E(M_B, \mathcal{D}_A)$.

3. \b{Performance Metrics}: 
   The performance of each model on the alternate dataset is calculated using evaluation metrics such as accuracy, precision, recall, or mean squared error (MSE), depending on the model's purpose. For instance, if mean squared error is used:
   \begin{align}
       \text{MSE}_{M_A \rightarrow B} &= \frac{1}{n_B} \sum_{i=1}^{n_B} \left(y_i^B - M_A(x_i^B)\right)^2, \\
       \text{MSE}_{M_B \rightarrow A} &= \frac{1}{n_A} \sum_{i=1}^{n_A} \left(y_i^A - M_B(x_i^A)\right)^2.
   \end{align}

The robustness of the models can be inferred by comparing $E(M_A, \mathcal{D}_A)$ and $E(M_A, \mathcal{D}_B)$ for $M_A$, and similarly, $E(M_B, \mathcal{D}_A)$ and $E(M_B, \mathcal{D}_B)$ for $M_B$. Consistent performance across both in-domain and out-of-domain evaluations suggests that the models have captured patterns that generalize well beyond the specific characteristics of their training data.

\subsection{Discussion of Cross-Dataset Validation Results}

By examining the cross-dataset performance of $M_A$ and $M_B$, we can validate the models' robustness and assess their ability to generalize across different datasets. This evaluation helps verify the model's capability to transfer learned representations and minimize dataset-specific biases, thereby enhancing the credibility of the model for broader applications.

\section{Resume}
In this chapter delves into the essential concepts and technological underpinnings relevant to the use of Large Language Models (LLMs) for processing and generating e-commerce product reviews. This chapter begins by discussing the role of sophisticated database technologies in e-commerce, highlighting how advanced querying and big data solutions enhance the management and utilization of product-related databases. Innovations like cloud computing and machine learning models are shown to significantly improve the efficiency and accuracy of product searches and trend analysis.
\\\\
The core focus of the chapter is on Large Language Models such as BERT and GPT, which represent a significant shift from statistical to neural network-based NLP models. These LLMs are extensively trained on vast datasets, enabling them to perform complex tasks like translation, summarization, and sentiment analysis. The chapter explains the process of fine-tuning these pretrained models on smaller, task-specific datasets, a method that optimizes them for specific applications by adjusting hyperparameters and using targeted training to improve performance and generalization.
\\\\
Additionally, the chapter discusses the integration of JSON-focused techniques for structuring data that LLMs process, aiming to refine the extraction and normalization of product specifications. This structured approach ensures the generation of accurate and contextually relevant product reviews, enhancing the user experience on e-commerce platforms .