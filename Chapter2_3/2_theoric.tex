\subsection*{Large Language Models (LLMs)}
Large language models (LLMs) represent significant progress in natural language processing (NLP), transitioning from statistical to neural models. The term "large language model" generally refers to pre-trained language models of substantial size, often containing hundreds of millions to billions of parameters \cite{zhao2023survey}.
\\\\
These models are trained on extensive text datasets using self-supervised learning techniques, enabling them to generate human-like text and perform tasks such as translation, summarization, and sentiment analysis. Due to their extensive training data and sophisticated architectures, LLMs can capture complex language patterns and demonstrate impressive zero-shot and few-shot learning capabilities \cite{naveed2024comprehensive}.
\\\\
Beyond typical NLP tasks, LLMs are utilized in various fields. They show potential in improving recommendation systems, executing complex planning, and contributing to areas like telecommunications and robotics \cite{10305960} \cite{fan2023fatellm}.

\subsection*{Fine tuning}
Fine-tuning large language models (LLMs) is a crucial process that involves adjusting the parameters of a pre-trained model to enhance its performance on specific downstream tasks. This method builds upon the extensive training done on massive, unlabeled text corpora, refining the model with a smaller, task-specific dataset. Fine-tuning is vital as it enables models to adapt from the broad, generic data of their initial training to the specialized tasks they are required to perform. For example, the Child-Tuning technique improves efficiency and performance by updating only a subset of parameters and masking out non-essential gradients, showing notable results on the GLUE benchmark \cite{xu-etal-2021-raise}.
\\\\
Fine-tuning strategies vary based on the model and available resources. Some approaches aim at parameter-efficient methods to reduce computational costs while maintaining high performance. Techniques like Low-Rank Adaptation (LoRA) allow extensive fine-tuning of LLMs with minimal additional parameters, making it feasible with limited computational resources \cite{sun2023comparative}. Additionally, methods such as differentially private fine-tuning have been developed to safeguard sensitive data during the fine-tuning process, balancing model utility and data privacy \cite{yu2022differentially}.

\subsection*{JSON-Tuning}
JSON-Tuning is a novel approach aimed at enhancing the performance and efficiency of Large Language Models (LLMs) by leveraging the structured data representation capabilities of JSON (JavaScript Object Notation). This method utilizes JSON's hierarchical structure to optimize the input-output processes of LLMs, leading to better parameter tuning and improved model interpretability. JSON-Tuning provides more precise control over training data, resulting in more robust and contextually accurate predictions. This approach also facilitates efficient data organization, simplifying management and utilization during the training and fine-tuning stages of LLM development \cite{zheng2024llamafactory}.
\\\\
The benefits of JSON-Tuning extend beyond performance improvements. This technique can substantially reduce the computational load typically associated with traditional fine-tuning methods. By streamlining data processing and minimizing redundancy, JSON-Tuning enables the deployment of LLMs in real-time applications where speed and accuracy are essential. Additionally, JSON's structured nature allows for seamless integration with existing data pipelines and APIs, simplifying workflows for data scientists and developers \cite{zhu2024lift}. This combination of structured data representation and advanced model tuning offers a promising avenue for future research and development in machine learning.

\subsection*{E-commerce Product-related Databases}
In the rapidly evolving world of e-commerce, managing and utilizing product-related databases has become more advanced. Recent developments focus on integrating sophisticated database queries and big data technologies to improve the efficiency and precision of product searches. Research indicates that incorporating database queries into e-commerce platforms significantly streamlines the search process, making it more user-friendly and effective \cite{Muntjir2016}. Additionally, using big data technologies like Hadoop and MPP distributed databases enables detailed analysis of customer reviews and purchasing trends, optimizing product selection and enhancing user experience \cite{Liang_2020}.
\\\\
The advancement of database technologies has also led to the creation of new frameworks that support complex data formats and improve the efficiency of e-commerce platforms. For instance, cloud computing-based platforms such as Productpedia help create a centralized electronic product catalog, allowing seamless data synchronization and enabling merchants to define and share semantically rich product information \cite{10.1007/978-3-319-20895-4_34}. Moreover, deploying machine learning models like TrendSpotter helps e-commerce platforms predict and highlight trending products by analyzing current customer engagement data, thereby meeting the market's dynamic demands \cite{10.1145/3583780.3615503}.