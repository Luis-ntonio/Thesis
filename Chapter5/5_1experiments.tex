%\todo[color=blue!40]{CS4003}
%\todo[color=green!40]{CS4004}
In this chapter, the results obtained from the implementation of the methodology described in the previous chapter are presented. First, the hyperparameters used for training the models are introduced. Subsequently, the results obtained by the models are presented. Finally, the evaluation of the models based on the evaluation metrics is shown, and the obtained results are discussed.

\section{Hyperparameters}
Table \ref{table:hyperparameters2} shows the hyperparameters used to train the models. As these are preliminary evaluations, the \textit{bitsandbytes} options used were those defined by an example of \href{https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html}{training an optimized LLM model}. For the rest of the hyperparameters, a default configuration was used.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Hyperparameter} & \textbf{Value} \\
        \hline
        Learning Rate & 2e-4 \\
        Batch Size & 2 \\
        Epochs & 1 \\
        max\_grad\_norm & 0.3 \\
        gradient\_accumulation\_steps & 1 \\
        weight\_decay & 0.001 \\
        warmup\_ratio & 0.03 \\
        lr\_scheduler\_type & cosine \\
        optim & adam \\
        max\_seq\_length & 900 \\
        bnb\_4bit\_compute\_dtype & float16 \\
        bnb\_4bit\_quant\_type & nf4 \\
        use\_nested\_quant & False \\
        \hline
    \end{tabular}
    \caption{Hyperparameters Selection}
    \label{table:hyperparameters2}
\end{table}

\subsection{Issues Encountered with the Development Environment}
During the training of the models, several issues were encountered with the development environment. Firstly, it was found that the Nvidia RTX 4070 Ti Super leaks in VRAM for the models if there where not quantizied. Secondly, the training time upscales 24h per model and more than 20h for testing each one. In order to find a solution for these problems it was necesary quantized the models to 4-bits.

\section{Experiments}
Table \ref{table:results} show the results obtained by the trained models. The results are presented in terms of BLEU, METEOR, ROUGE, faithfullness, and correctness scores. 
\begin{table}[H]
    \begin{adjustwidth}{-7em}{}
    \begin{tblr}{hline{1,2,Z} = 0.8pt, hline{3-Y} = 0.2pt, vlines,
                 colspec = {Q[c,m, 3.5em] Q[c,m, 7em] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}] Q[c,m, 3.5em, font=\fontfamily{qcr}]},
                 colsep  = 4pt,
                 row{1}  = {font=\bfseries, bg=gray!30},
                 row{2-Z} = {1.5cm},
                 }
    
        Type & Model & Bleu & Meteor & Rouge-1 & Rouge-2 & Rouge-L & RougeL-sum & Correct\newline ness & Faith-\newline fulness \\
    \SetCell[r=3]{c} Trained
    & LLAMA2 & \textcolor{red}{\textbf{29.36\%}} & 40.2\% & 48.36\% & 25.7\% & 39.19\% & 39.19\% & 61.53\% & 63.33\% \\
    %
    & StructLM & 31.06\% & 42.3\% & 49.42\% & 27.29\% & 40.58\% & 40.58\% & 69.71\% & 72.16\% \\
    %
    & Mistral\_Instruct & 38.89\% & 49.43\% & 56.64\% & 35.51\% & 48.32\% & 48.32\% & 73.50\% & 76.92\% \\
    \SetCell[r=3]{c} Base
    & LLAMA2 & 1.39\% & 3.59\% & 5.57\% & 1.84\% & 4.09\% & 4.09\% & 33.18\% & 33.67\% \\
    %
    & StructLM & 6.21\% & 11.96\% & 20.09\% & 7.72\% & 15.34\% & 15.34\% & 65.14\% & 70.50\% \\
    %
    & Mistral\_Instruct & 4.19\% & 9.55\% & 25.64\% & 10.4\% & 18.99\% & 18.99\% & 77.96\% & 82.51\% \\
    \end{tblr}
    \end{adjustwidth}
    \caption{Results of Trained vs. Base Models: LLAMA2, StructLM, and Mistral\_Instruct}
    \label{table:results}
\end{table}
    
As observed in the results, all the models demonstrate strong metrics for matching evaluation, particularly considering that their primary purpose is to generate human-like sentences. This characteristic complicates word-by-word evaluation. However, the metrics used to assess sentence responses faithfulness and correctness indicate favorable outcomes, with particularly strong performance from the Mistral\_Instruct model.
%As can be seen in the results, the percentage performance of the trained models is superior to that of the base models. In the case of the LLAMA2 model, the performance of the trained model is 37.21 times superior to that of the base model. In the case of the StructLM model, the performance of the trained model is 45.11 times superior to that of the base model. Finally, in the case of the Mistral\_Instruct model, the performance of the trained model is 50.09 times superior to that of the base model. These results show that training the models with the consumer technology product dataset significantly improves their performance. 
\subsection{Discussion}
In the experiment, the trained models demonstrated partially positive results in generating reviews based on word-level metrics, with Mistral\_Instruct standing out. However, the model-based scores indicate that the LLMs successfully detected the key fields in the JSON specifications and generated accurate responses. Mistral\_Instruct outperformed the other models, particularly surpassing LLAMA2. This may be due to the fact that LLAMA2 was not initially designed for instruction-based queries.

\subsection{Resume}
This section outlines the experimental setup used to evaluate the proposed methodologies, including details about the hyperparameters and configurations of the trained models. The primary focus was to assess the performance differences between the base models and the specifically trained models using various metrics such as BLEU, METEOR, ROUGE, faithfullness and correctness scores. The experiments demonstrated significant improvements in the trained models all metrics, showcasing the effectiveness of the training process tailored to the consumer technology product dataset.