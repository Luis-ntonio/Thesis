@misc{Bergmann_2024,
  title   = {Build an LLM-Powered Data Agent for Data Analysis},
  url     = {https://www.ibm.com/topics/fine-tuning},
  journal = {IBM},
  author  = {Bergmann, Dave},
  year    = {2024},
  month   = {March}
}

@misc{gao2024jsontuning,
  title         = {JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning},
  author        = {Chang Gao and Wenxuan Zhang and Guizhen Chen and Wai Lam},
  year          = {2024},
  eprint        = {2310.02953},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{he2023survey,
  title         = {A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics},
  author        = {Kai He and Rui Mao and Qika Lin and Yucheng Ruan and Xiang Lan and Mengling Feng and Erik Cambria},
  year          = {2023},
  eprint        = {2310.05694},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
} 

@misc{jiang2023mistral,
  title         = {Mistral 7B},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2023},
  eprint        = {2310.06825},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
} 

@misc{liu2023reviewergpt,
  title         = {ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing},
  author        = {Ryan Liu and Nihar B. Shah},
  year          = {2023},
  eprint        = {2306.00622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{REDDY2023101304,
  title    = {Evaluating large language models for use in healthcare: A framework for translational value assessment},
  journal  = {Informatics in Medicine Unlocked},
  volume   = {41},
  pages    = {101304},
  year     = {2023},
  issn     = {2352-9148},
  doi      = {https://doi.org/10.1016/j.imu.2023.101304},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352914823001508},
  author   = {Sandeep Reddy},
  abstract = {The recent focus on Large Language Models (LLMs) has yielded unprecedented discussion of their potential use in various domains, including healthcare. While showing considerable potential in performing human-capable tasks, LLMs have also demonstrated significant drawbacks, including generating misinformation, falsifying data, and contributing to plagiarism. These aspects are generally concerning but can be more severe in the context of healthcare. As LLMs are explored for utility in healthcare, including generating discharge summaries, interpreting medical records and providing medical advice, it is necessary to ensure safeguards around their use in healthcare. Notably, there must be an evaluation process that assesses LLMs for their natural language processing performance and their translational value. Complementing this assessment, a governance layer can ensure accountability and public confidence in such models. Such an evaluation framework is discussed and presented in this paper.}
}

@misc{singha2023tabular,
  title         = {Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs},
  author        = {Ananya Singha and José Cambronero and Sumit Gulwani and Vu Le and Chris Parnin},
  year          = {2023},
  eprint        = {2310.10358},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{touvron2023llama,
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year          = {2023},
  eprint        = {2307.09288},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{Varshney_2024,
  title   = {Build an LLM-Powered Data Agent for Data Analysis},
  url     = {https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/},
  journal = {Nvidia Developer},
  author  = {Varshney, Tanay},
  year    = {2024},
  month   = {Feb}
}

@misc{wu2023survey,
  title         = {A Survey on Large Language Models for Recommendation},
  author        = {Likang Wu and Zhi Zheng and Zhaopeng Qiu and Hao Wang and Hongchao Gu and Tingjia Shen and Chuan Qin and Chen Zhu and Hengshu Zhu and Qi Liu and Hui Xiong and Enhong Chen},
  year          = {2023},
  eprint        = {2305.19860},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{zhuang2024structlm,
  title         = {StructLM: Towards Building Generalist Models for Structured Knowledge Grounding},
  author        = {Alex Zhuang and Ge Zhang and Tianyu Zheng and Xinrun Du and Junjie Wang and Weiming Ren and Stephen W. Huang and Jie Fu and Xiang Yue and Wenhu Chen},
  year          = {2024},
  eprint        = {2402.16671},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}