@misc{Bergmann_2024,
  title   = {Build an LLM-Powered Data Agent for Data Analysis},
  url     = {https://www.ibm.com/topics/fine-tuning},
  journal = {IBM},
  author  = {Bergmann, Dave},
  year    = {2024},
  month   = {March}
}

@misc{gao2024jsontuning,
  title         = {JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning},
  author        = {Chang Gao and Wenxuan Zhang and Guizhen Chen and Wai Lam},
  year          = {2024},
  eprint        = {2310.02953},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{he2023survey,
  title         = {A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics},
  author        = {Kai He and Rui Mao and Qika Lin and Yucheng Ruan and Xiang Lan and Mengling Feng and Erik Cambria},
  year          = {2023},
  eprint        = {2310.05694},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
} 

@misc{jiang2023mistral,
  title         = {Mistral 7B},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2023},
  eprint        = {2310.06825},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
} 

@misc{liu2023reviewergpt,
  title         = {ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing},
  author        = {Ryan Liu and Nihar B. Shah},
  year          = {2023},
  eprint        = {2306.00622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{naveed2024comprehensive,
  title         = {A Comprehensive Overview of Large Language Models},
  author        = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
  year          = {2024},
  eprint        = {2307.06435},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{REDDY2023101304,
  title    = {Evaluating large language models for use in healthcare: A framework for translational value assessment},
  journal  = {Informatics in Medicine Unlocked},
  volume   = {41},
  pages    = {101304},
  year     = {2023},
  issn     = {2352-9148},
  doi      = {https://doi.org/10.1016/j.imu.2023.101304},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352914823001508},
  author   = {Sandeep Reddy},
  abstract = {The recent focus on Large Language Models (LLMs) has yielded unprecedented discussion of their potential use in various domains, including healthcare. While showing considerable potential in performing human-capable tasks, LLMs have also demonstrated significant drawbacks, including generating misinformation, falsifying data, and contributing to plagiarism. These aspects are generally concerning but can be more severe in the context of healthcare. As LLMs are explored for utility in healthcare, including generating discharge summaries, interpreting medical records and providing medical advice, it is necessary to ensure safeguards around their use in healthcare. Notably, there must be an evaluation process that assesses LLMs for their natural language processing performance and their translational value. Complementing this assessment, a governance layer can ensure accountability and public confidence in such models. Such an evaluation framework is discussed and presented in this paper.}
}

@misc{singha2023tabular,
  title         = {Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs},
  author        = {Ananya Singha and José Cambronero and Sumit Gulwani and Vu Le and Chris Parnin},
  year          = {2023},
  eprint        = {2310.10358},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{touvron2023llama,
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year          = {2023},
  eprint        = {2307.09288},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{Varshney_2024,
  title   = {Build an LLM-Powered Data Agent for Data Analysis},
  url     = {https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/},
  journal = {Nvidia Developer},
  author  = {Varshney, Tanay},
  year    = {2024},
  month   = {Feb}
}

@misc{wu2023survey,
  title         = {A Survey on Large Language Models for Recommendation},
  author        = {Likang Wu and Zhi Zheng and Zhaopeng Qiu and Hao Wang and Hongchao Gu and Tingjia Shen and Chuan Qin and Chen Zhu and Hengshu Zhu and Qi Liu and Hui Xiong and Enhong Chen},
  year          = {2023},
  eprint        = {2305.19860},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{zhao2023survey,
  title         = {A Survey of Large Language Models},
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  eprint        = {2303.18223},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhuang2024structlm,
  title         = {StructLM: Towards Building Generalist Models for Structured Knowledge Grounding},
  author        = {Alex Zhuang and Ge Zhang and Tianyu Zheng and Xinrun Du and Junjie Wang and Weiming Ren and Stephen W. Huang and Jie Fu and Xiang Yue and Wenhu Chen},
  year          = {2024},
  eprint        = {2402.16671},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{fan2023fatellm,
      title={FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models}, 
      author={Tao Fan and Yan Kang and Guoqiang Ma and Weijing Chen and Wenbin Wei and Lixin Fan and Qiang Yang},
      year={2023},
      eprint={2310.10049},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{10305960,
  author={Debbah, Mérouane},
  booktitle={2023 Eighth International Conference on Fog and Mobile Edge Computing (FMEC)}, 
  title={Large Language Models for Telecom}, 
  year={2023},
  volume={},
  number={},
  pages={3-4},
  keywords={Fault diagnosis;Sentiment analysis;Multi-access edge computing;Wireless networks;Computational modeling;Network security;Telecommunications},
  doi={10.1109/FMEC59375.2023.10305960}}

@inproceedings{xu-etal-2021-raise,
    title = "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning",
    author = "Xu, Runxin  and
      Luo, Fuli  and
      Zhang, Zhiyuan  and
      Tan, Chuanqi  and
      Chang, Baobao  and
      Huang, Songfang  and
      Huang, Fei",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.749",
    doi = "10.18653/v1/2021.emnlp-main.749",
    pages = "9514--9528",
    abstract = "Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.",
}

@misc{sun2023comparative,
      title={A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model}, 
      author={Xianghui Sun and Yunjie Ji and Baochang Ma and Xiangang Li},
      year={2023},
      eprint={2304.08109},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yu2022differentially,
      title={Differentially Private Fine-tuning of Language Models}, 
      author={Da Yu and Saurabh Naik and Arturs Backurs and Sivakanth Gopi and Huseyin A. Inan and Gautam Kamath and Janardhan Kulkarni and Yin Tat Lee and Andre Manoel and Lukas Wutschitz and Sergey Yekhanin and Huishuai Zhang},
      year={2022},
      eprint={2110.06500},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zheng2024llamafactory,
      title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models}, 
      author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},
      year={2024},
      eprint={2403.13372},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhu2024lift,
title={{LIFT}: Efficient Layer-wise Fine-tuning for Large Model Models},
author={Ligeng Zhu and Lanxiang Hu and Ji Lin and Song Han},
year={2024},
url={https://openreview.net/forum?id=u0INlprg3U}
}

@misc{macková2023promap,
      title={ProMap: Datasets for Product Mapping in E-commerce}, 
      author={Kateřina Macková and Martin Pilát},
      year={2023},
      eprint={2309.06882},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{10.1007/978-3-319-20895-4_34,
author="Tan, Wee-Kek
and Teo, Hock-Hai",
editor="Fui-Hoon Nah, Fiona
and Tan, Chuan-Hoo",
title="Productpedia -- A Collaborative Electronic Product Catalog for Ecommerce 3.0",
booktitle="HCI in Business",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="370--381",
abstract="Despite the advancements made in ecommerce technologies over the past years, the inability to define and exchange semantically rich and accurate product information among ecommerce websites/applications has continued to intrigue researchers. This problem has taken on greater urgency because it impedes the realization of the full benefits of Ecommerce 3.0. The present research conceptualizes, designs and implements a cloud computing-based platform that enables global merchants to maintain a collaborative Electronic Product Catalog (EPC) known as Productpedia. This collaborative EPC platform addresses numerous shortcomings of prior researches by (1) maintaining a single centralized EPC database; (2) negating the need to synchronize and convert data; (3) creating an integrated meta-model ontology for merchants to define previously unclassified product information without the involvement of domain experts; and (4) enabling an Open Application Programming Interface based on RESTful web services to facilitate direct modification of the EPC database by even third-party applications.",
isbn="978-3-319-20895-4"
}

@inproceedings{10.1145/3583780.3615503,
author = {Ryali, Gayatri and S, Shreyas and Kaveri, Sivaramakrishnan and Comar, Prakash Mandayam},
title = {TrendSpotter: Forecasting E-commerce Product Trends},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615503},
doi = {10.1145/3583780.3615503},
abstract = {Internet users actively search for trending products on various social media services like Instagram and YouTube which serve as popular hubs for discovering and exploring fashionable and popular items. It is imperative for e-commerce giants to have the capability to accurately identify, predict and subsequently showcase these trending products to the customers. E-commerce stores can effectively cater to the evolving demands of the customer base and enhance the overall shopping experience by offering recent and most sought-after products in a timely manner. In this work we propose a framework for predicting and surfacing trending products in e-commerce stores, the first of its kind to the best of our knowledge. We begin by defining what constitutes a trending product using sound statistical tests. We then introduce a machine learning-based early trend prediction system called TrendSpotter to help users identify upcoming product trends. TrendSpotter is a unique adaptation of the state-of-the-art InceptionTime modelciteInceptionTime that predicts the future popularity of a product based on its current customer engagement, such as clicks, purchases, and other relevant product attributes. The effectiveness of our approach is demonstrated through A/B tests, where we first showcase the effectiveness of our statistical test based labeling strategy, resulting in an incremental sales lift of 59 bpsfootnotebps or basis points are a measure of percentages. 1 bps = 0.01\% across two experiments on home page and search page. Subsequently, we conduct a comparison between our machine learning model and the statistical labeling baseline and observe an additional sales gain of 14 bps, reflecting the importance of early identification of trending products.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4808–4814},
numpages = {7},
keywords = {trends, time series, e-commerce, convolutional neural networks},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@article{Liang_2020,
doi = {10.1088/1742-6596/1601/3/032012},
url = {https://dx.doi.org/10.1088/1742-6596/1601/3/032012},
year = {2020},
month = {jul},
publisher = {IOP Publishing},
volume = {1601},
number = {3},
pages = {032012},
author = {Jia-Hui Liang},
title = {Application of Big Data Technology in Product Selection on Cross-border E-commerce Platforms},
journal = {Journal of Physics: Conference Series},
abstract = {With Amazon as the study case, the application of big data in product selection on this e-commerce platform is studied. Two big data analysis tools commonly used in commerce, the MPP distributed database and the Hadoop distributed database, were analyzed. Based on big data technology, the search function of the platform, the analytical tools, and third-party data analytical tools, this study compared different levels of comments of customers for the same type of products and analyzed the product selection mechanism.}
}

@article{Muntjir2016,
title = {An Enhanced Framework with Advanced Study to Incorporate the Searching of E-Commerce Products Using Modernization of Database Queries},
journal = {International Journal of Advanced Computer Science and Applications},
doi = {10.14569/IJACSA.2016.070514},
url = {http://dx.doi.org/10.14569/IJACSA.2016.070514},
year = {2016},
publisher = {The Science and Information Organization},
volume = {7},
number = {5},
author = {Mohd Muntjir and Ahmad Tasnim Siddiqui}
}

@inproceedings{chen-etal-2022-bert2bert,
    title = "bert2{BERT}: Towards Reusable Pretrained Language Models",
    author = "Chen, Cheng  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Qin, Yujia  and
      Wang, Fengyu  and
      Wang, Zhi  and
      Chen, Xiao  and
      Liu, Zhiyuan  and
      Liu, Qun",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.151",
    doi = "10.18653/v1/2022.acl-long.151",
    pages = "2134--2148",
    abstract = "In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model{'}s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45{\%} and 47{\%} computational cost of pre-training BERT$_{\rm BASE}$ and GPT$_{\rm BASE}$ by reusing the models of almost their half sizes.",
}

@misc{agrawal2022large,
      title={Large Language Models are Few-Shot Clinical Information Extractors}, 
      author={Monica Agrawal and Stefan Hegselmann and Hunter Lang and Yoon Kim and David Sontag},
      year={2022},
      eprint={2205.12689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{edunov-etal-2019-pre,
    title = "Pre-trained language model representations for language generation",
    author = "Edunov, Sergey  and
      Baevski, Alexei  and
      Auli, Michael",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1409",
    doi = "10.18653/v1/N19-1409",
    pages = "4052--4059",
    abstract = "Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14{\%}. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.",
}

@misc{brinkmann2024product,
      title={Product Attribute Value Extraction using Large Language Models}, 
      author={Alexander Brinkmann and Roee Shraga and Christian Bizer},
      year={2024},
      eprint={2310.12537},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{labrak2024biomistral,
      title={BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains}, 
      author={Yanis Labrak and Adrien Bazoge and Emmanuel Morin and Pierre-Antoine Gourraud and Mickael Rouvier and Richard Dufour},
      year={2024},
      eprint={2402.10373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{skondras2023generating,
  title={Generating Synthetic Resume Data with Large Language Models for Enhanced Job Description Classification},
  author={Skondras, Panagiotis and Zervas, Panagiotis and Tzimas, Giannis},
  journal={Future Internet},
  volume={15},
  number={11},
  pages={363},
  year={2023},
  publisher={MDPI}
}

@misc{tang2024strucbench,
      title={Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?}, 
      author={Xiangru Tang and Yiming Zong and Jason Phang and Yilun Zhao and Wangchunshu Zhou and Arman Cohan and Mark Gerstein},
      year={2024},
      eprint={2309.08963},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2024emerging,
      title={Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations}, 
      author={Xiaonan Xu and Yichao Wu and Penghao Liang and Yuhang He and Han Wang},
      year={2024},
      eprint={2403.02760},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{zhang2021ebert,
      title={E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce}, 
      author={Denghui Zhang and Zixuan Yuan and Yanchi Liu and Fuzhen Zhuang and Haifeng Chen and Hui Xiong},
      year={2021},
      eprint={2009.02835},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2023leveraging,
      title={Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce}, 
      author={Jianghong Zhou and Bo Liu and Jhalak Nilesh Acharya Yao Hong and Kuang-chih Lee and Musen Wen},
      year={2023},
      eprint={2310.18357},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yuan2023scaling,
      title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models}, 
      author={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.01825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{creswell2022selectioninference,
      title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning}, 
      author={Antonia Creswell and Murray Shanahan and Irina Higgins},
      year={2022},
      eprint={2205.09712},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{zhou-etal-2023-inform,
    title = "{INFORM} : Information e{N}tropy based multi-step reasoning {FOR} large language Models",
    author = "Zhou, Chuyue  and
      You, Wangjie  and
      Li, Juntao  and
      Ye, Jing  and
      Chen, Kehai  and
      Zhang, Min",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.216",
    doi = "10.18653/v1/2023.emnlp-main.216",
    pages = "3565--3576",
    abstract = "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency.",
}

@misc{minaee2024large,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024largescale,
      title={Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey}, 
      author={Xiao Wang and Guangyao Chen and Guangwu Qian and Pengcheng Gao and Xiao-Yong Wei and Yaowei Wang and Yonghong Tian and Wen Gao},
      year={2024},
      eprint={2302.10035},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}