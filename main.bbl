% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{he2023survey}
K.~He, R.~Mao, Q.~Lin, Y.~Ruan, X.~Lan, M.~Feng, and E.~Cambria, ``A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics,'' 2023.

\bibitem{REDDY2023101304}
\BIBentryALTinterwordspacing
S.~Reddy, ``Evaluating large language models for use in healthcare: A framework for translational value assessment,'' \emph{Informatics in Medicine Unlocked}, vol.~41, p. 101304, 2023. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S2352914823001508}
\BIBentrySTDinterwordspacing

\bibitem{Varshney_2024}
\BIBentryALTinterwordspacing
T.~Varshney, ``Build an llm-powered data agent for data analysis,'' Feb 2024. [Online]. Available: \url{https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/}
\BIBentrySTDinterwordspacing

\bibitem{Bergmann_2024}
\BIBentryALTinterwordspacing
D.~Bergmann, ``Build an llm-powered data agent for data analysis,'' March 2024. [Online]. Available: \url{https://www.ibm.com/topics/fine-tuning}
\BIBentrySTDinterwordspacing

\bibitem{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C. Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom, ``Llama 2: Open foundation and fine-tuned chat models,'' 2023.

\bibitem{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~de~las Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R. Lavaud, M.-A. Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and W.~E. Sayed, ``Mistral 7b,'' 2023.

\bibitem{zhuang2024structlm}
A.~Zhuang, G.~Zhang, T.~Zheng, X.~Du, J.~Wang, W.~Ren, S.~W. Huang, J.~Fu, X.~Yue, and W.~Chen, ``Structlm: Towards building generalist models for structured knowledge grounding,'' 2024.

\bibitem{singha2023tabular}
A.~Singha, J.~Cambronero, S.~Gulwani, V.~Le, and C.~Parnin, ``Tabular representation, noisy operators, and impacts on table structure understanding tasks in llms,'' 2023.

\bibitem{gao2024jsontuning}
C.~Gao, W.~Zhang, G.~Chen, and W.~Lam, ``Jsontuning: Towards generalizable, robust, and controllable instruction tuning,'' 2024.

\bibitem{mumtaz2024llmshealthcare}
U.~Mumtaz, A.~Ahmed, and S.~Mumtaz, ``Llms-healthcare : Current applications and challenges of large language models in various medical specialties,'' 2024.

\bibitem{10.1093/bioinformatics/btz682}
\BIBentryALTinterwordspacing
J.~Lee, W.~Yoon, S.~Kim, D.~Kim, S.~Kim, C.~H. So, and J.~Kang, ``{BioBERT: a pre-trained biomedical language representation model for biomedical text mining},'' \emph{Bioinformatics}, vol.~36, no.~4, pp. 1234--1240, 09 2019. [Online]. Available: \url{https://doi.org/10.1093/bioinformatics/btz682}
\BIBentrySTDinterwordspacing

\bibitem{zhao2024revolutionizing}
H.~Zhao, Z.~Liu, Z.~Wu, Y.~Li, T.~Yang, P.~Shu, S.~Xu, H.~Dai, L.~Zhao, G.~Mai, N.~Liu, and T.~Liu, ``Revolutionizing finance with llms: An overview of applications and insights,'' 2024.

\bibitem{macková2023promap}
K.~Macková and M.~Pilát, ``Promap: Datasets for product mapping in e-commerce,'' 2023.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert pretraining approach,'' 2019.

\bibitem{rfc8259}
\BIBentryALTinterwordspacing
T.~Bray, ``{The JavaScript Object Notation (JSON) Data Interchange Format},'' RFC 8259, Dec. 2017. [Online]. Available: \url{https://www.rfc-editor.org/info/rfc8259}
\BIBentrySTDinterwordspacing

\bibitem{ling2024domain}
C.~Ling, X.~Zhao, J.~Lu, C.~Deng, C.~Zheng, J.~Wang, T.~Chowdhury, Y.~Li, H.~Cui, X.~Zhang, T.~Zhao, A.~Panalkar, D.~Mehta, S.~Pasquali, W.~Cheng, H.~Wang, Y.~Liu, Z.~Chen, H.~Chen, C.~White, Q.~Gu, J.~Pei, C.~Yang, and L.~Zhao, ``Domain specialization as the key to make large language models disruptive: A comprehensive survey,'' 2024.

\bibitem{vaswani2023attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' 2023.

\bibitem{Wang2023Emotional}
X.~Wang, X.~Li, Z.~Yin, Y.~Wu, L.~J.~D. of~PsychologyTsinghua Laboratory~of Brain, Intelligence, T.~University, D.~Psychology, and R.~University, ``Emotional intelligence of large language models,'' \emph{ArXiv}, vol. abs/2307.09042, 2023.

\bibitem{Duong2023Analysis}
D.~Duong and B.~D. Solomon, ``Analysis of large-language model versus human performance for genetics questions,'' \emph{medRxiv : the preprint server for health sciences}, 2023.

\bibitem{Suri2023Do}
G.~Suri, L.~R. Slater, A.~Ziaee, and M.~Nguyen, ``Do large language models show decision heuristics similar to humans? a case study using gpt-3.5,'' \emph{ArXiv}, vol. abs/2305.04400, 2023.

\bibitem{Muntjir2016}
\BIBentryALTinterwordspacing
M.~Muntjir and A.~T. Siddiqui, ``An enhanced framework with advanced study to incorporate the searching of e-commerce products using modernization of database queries,'' \emph{International Journal of Advanced Computer Science and Applications}, vol.~7, no.~5, 2016. [Online]. Available: \url{http://dx.doi.org/10.14569/IJACSA.2016.070514}
\BIBentrySTDinterwordspacing

\bibitem{Liang_2020}
\BIBentryALTinterwordspacing
J.-H. Liang, ``Application of big data technology in product selection on cross-border e-commerce platforms,'' \emph{Journal of Physics: Conference Series}, vol. 1601, no.~3, p. 032012, jul 2020. [Online]. Available: \url{https://dx.doi.org/10.1088/1742-6596/1601/3/032012}
\BIBentrySTDinterwordspacing

\bibitem{10.1007/978-3-319-20895-4_34}
W.-K. Tan and H.-H. Teo, ``Productpedia -- a collaborative electronic product catalog for ecommerce 3.0,'' in \emph{HCI in Business}, F.~Fui-Hoon~Nah and C.-H. Tan, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Cham: Springer International Publishing, 2015, pp. 370--381.

\bibitem{10.1145/3583780.3615503}
\BIBentryALTinterwordspacing
G.~Ryali, S.~S, S.~Kaveri, and P.~M. Comar, ``Trendspotter: Forecasting e-commerce product trends,'' in \emph{Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}, ser. CIKM '23.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2023, p. 4808–4814. [Online]. Available: \url{https://doi.org/10.1145/3583780.3615503}
\BIBentrySTDinterwordspacing

\bibitem{zhao2023survey}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang, J.~Zhang, Z.~Dong, Y.~Du, C.~Yang, Y.~Chen, Z.~Chen, J.~Jiang, R.~Ren, Y.~Li, X.~Tang, Z.~Liu, P.~Liu, J.-Y. Nie, and J.-R. Wen, ``A survey of large language models,'' 2023.

\bibitem{naveed2024comprehensive}
H.~Naveed, A.~U. Khan, S.~Qiu, M.~Saqib, S.~Anwar, M.~Usman, N.~Akhtar, N.~Barnes, and A.~Mian, ``A comprehensive overview of large language models,'' 2024.

\bibitem{10305960}
M.~Debbah, ``Large language models for telecom,'' in \emph{2023 Eighth International Conference on Fog and Mobile Edge Computing (FMEC)}, 2023, pp. 3--4.

\bibitem{fan2023fatellm}
T.~Fan, Y.~Kang, G.~Ma, W.~Chen, W.~Wei, L.~Fan, and Q.~Yang, ``Fate-llm: A industrial grade federated learning framework for large language models,'' 2023.

\bibitem{Lalor2017Improving}
J.~P. Lalor, H.~Wu, and H.~Yu, ``Improving machine learning ability with fine-tuning,'' \emph{ArXiv}, vol. abs/1702.08563, 2017.

\bibitem{Catani2020A}
L.~Catani and M.~Leifer, ``A mathematical framework for operational fine tunings,'' \emph{Quantum}, vol.~7, p. 948, 2020.

\bibitem{Shachaf2021A}
G.~Shachaf, A.~Brutzkus, and A.~Globerson, ``A theoretical analysis of fine-tuning with linear teachers,'' \emph{ArXiv}, 2021.

\bibitem{Vrbancic2020Transfer}
G.~Vrbancic and V.~Podgorelec, ``Transfer learning with adaptive fine-tuning,'' \emph{IEEE Access}, vol.~8, pp. 196\,197--196\,211, 2020.

\bibitem{Xiao2023Offsite-Tuning:}
G.~Xiao, J.~Lin, and S.~Han, ``Offsite-tuning: Transfer learning without full model,'' \emph{ArXiv}, vol. abs/2302.04870, 2023.

\bibitem{zheng2024llamafactory}
Y.~Zheng, R.~Zhang, J.~Zhang, Y.~Ye, Z.~Luo, and Y.~Ma, ``Llamafactory: Unified efficient fine-tuning of 100+ language models,'' 2024.

\bibitem{zhu2024lift}
\BIBentryALTinterwordspacing
L.~Zhu, L.~Hu, J.~Lin, and S.~Han, ``{LIFT}: Efficient layer-wise fine-tuning for large model models,'' 2024. [Online]. Available: \url{https://openreview.net/forum?id=u0INlprg3U}
\BIBentrySTDinterwordspacing

\bibitem{Reiter2018A}
E.~Reiter, ``A structured review of the validity of bleu,'' \emph{Computational Linguistics}, vol. Just Accepted, pp. 1--8, 2018.

\bibitem{Ng2015Better}
J.-P. Ng and V.~Abrecht, ``Better summarization evaluation with word embeddings for rouge,'' \emph{ArXiv}, vol. abs/1508.06034, 2015.

\bibitem{Ganesan2015ROUGE}
K.~A. Ganesan, ``Rouge 2.0: Updated and improved measures for evaluation of summarization tasks,'' \emph{ArXiv}, vol. abs/1803.01937, 2015.

\bibitem{Agarwal2008Meteor}
A.~Agarwal and A.~Lavie, ``Meteor, m-bleu and m-ter: Evaluation metrics for high-correlation with human rankings of machine translation output,'' \emph{ArXiv}, pp. 115--118, 2008.

\bibitem{lyu2024faithfulmodelexplanationnlp}
\BIBentryALTinterwordspacing
Q.~Lyu, M.~Apidianaki, and C.~Callison-Burch, ``Towards faithful model explanation in nlp: A survey,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2209.11326}
\BIBentrySTDinterwordspacing

\bibitem{madsen-etal-2022-evaluating}
\BIBentryALTinterwordspacing
A.~Madsen, N.~Meade, V.~Adlakha, and S.~Reddy, ``Evaluating the faithfulness of importance measures in {NLP} by recursively masking allegedly important tokens and retraining,'' in \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, Y.~Goldberg, Z.~Kozareva, and Y.~Zhang, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 1731--1751. [Online]. Available: \url{https://aclanthology.org/2022.findings-emnlp.125}
\BIBentrySTDinterwordspacing

\bibitem{yin2022sensitivitystabilitymodelinterpretations}
\BIBentryALTinterwordspacing
F.~Yin, Z.~Shi, C.-J. Hsieh, and K.-W. Chang, ``On the sensitivity and stability of model interpretations in nlp,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2104.08782}
\BIBentrySTDinterwordspacing

\bibitem{parcalabescu2024measuringfaithfulnessselfconsistencynatural}
\BIBentryALTinterwordspacing
L.~Parcalabescu and A.~Frank, ``On measuring faithfulness or self-consistency of natural language explanations,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2311.07466}
\BIBentrySTDinterwordspacing

\bibitem{kim2024prometheus2opensource}
\BIBentryALTinterwordspacing
S.~Kim, J.~Suk, S.~Longpre, B.~Y. Lin, J.~Shin, S.~Welleck, G.~Neubig, M.~Lee, K.~Lee, and M.~Seo, ``Prometheus 2: An open source language model specialized in evaluating other language models,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2405.01535}
\BIBentrySTDinterwordspacing

\bibitem{gat2023faithfulexplanationsblackboxnlp}
\BIBentryALTinterwordspacing
Y.~Gat, N.~Calderon, A.~Feder, A.~Chapanin, A.~Sharma, and R.~Reichart, ``Faithful explanations of black-box nlp models using llm-generated counterfactuals,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2310.00603}
\BIBentrySTDinterwordspacing

\bibitem{jacovi-goldberg-2020-towards}
\BIBentryALTinterwordspacing
A.~Jacovi and Y.~Goldberg, ``Towards faithfully interpretable {NLP} systems: How should we define and evaluate faithfulness?'' in \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, D.~Jurafsky, J.~Chai, N.~Schluter, and J.~Tetreault, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Online: Association for Computational Linguistics, Jul. 2020, pp. 4198--4205. [Online]. Available: \url{https://aclanthology.org/2020.acl-main.386}
\BIBentrySTDinterwordspacing

\bibitem{yao2023predictinggeneralizationperformancecorrectness}
\BIBentryALTinterwordspacing
Y.~Yao and A.~Koller, ``Predicting generalization performance with correctness discriminators,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2311.09422}
\BIBentrySTDinterwordspacing

\bibitem{varshney-etal-2022-towards}
\BIBentryALTinterwordspacing
N.~Varshney, S.~Mishra, and C.~Baral, ``Towards improving selective prediction ability of {NLP} systems,'' in \emph{Proceedings of the 7th Workshop on Representation Learning for NLP}, S.~Gella, H.~He, B.~P. Majumder, B.~Can, E.~Giunchiglia, S.~Cahyawijaya, S.~Min, M.~Mozes, X.~L. Li, I.~Augenstein, A.~Rogers, K.~Cho, E.~Grefenstette, L.~Rimell, and C.~Dyer, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 221--226. [Online]. Available: \url{https://aclanthology.org/2022.repl4nlp-1.23}
\BIBentrySTDinterwordspacing

\bibitem{steen2023littlepushnlimodels}
\BIBentryALTinterwordspacing
J.~Steen, J.~Opitz, A.~Frank, and K.~Markert, ``With a little push, nli models can robustly and efficiently predict faithfulness,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2305.16819}
\BIBentrySTDinterwordspacing

\bibitem{chen-etal-2022-bert2bert}
\BIBentryALTinterwordspacing
C.~Chen, Y.~Yin, L.~Shang, X.~Jiang, Y.~Qin, F.~Wang, Z.~Wang, X.~Chen, Z.~Liu, and Q.~Liu, ``bert2{BERT}: Towards reusable pretrained language models,'' in \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, S.~Muresan, P.~Nakov, and A.~Villavicencio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 2134--2148. [Online]. Available: \url{https://aclanthology.org/2022.acl-long.151}
\BIBentrySTDinterwordspacing

\bibitem{agrawal2022large}
M.~Agrawal, S.~Hegselmann, H.~Lang, Y.~Kim, and D.~Sontag, ``Large language models are few-shot clinical information extractors,'' 2022.

\bibitem{edunov-etal-2019-pre}
\BIBentryALTinterwordspacing
S.~Edunov, A.~Baevski, and M.~Auli, ``Pre-trained language model representations for language generation,'' in \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4052--4059. [Online]. Available: \url{https://aclanthology.org/N19-1409}
\BIBentrySTDinterwordspacing

\bibitem{brinkmann2024product}
A.~Brinkmann, R.~Shraga, and C.~Bizer, ``Product attribute value extraction using large language models,'' 2024.

\bibitem{labrak2024biomistral}
Y.~Labrak, A.~Bazoge, E.~Morin, P.-A. Gourraud, M.~Rouvier, and R.~Dufour, ``Biomistral: A collection of open-source pretrained large language models for medical domains,'' 2024.

\bibitem{skondras2023generating}
P.~Skondras, P.~Zervas, and G.~Tzimas, ``Generating synthetic resume data with large language models for enhanced job description classification,'' \emph{Future Internet}, vol.~15, no.~11, p. 363, 2023.

\bibitem{tang2024strucbench}
X.~Tang, Y.~Zong, J.~Phang, Y.~Zhao, W.~Zhou, A.~Cohan, and M.~Gerstein, ``Struc-bench: Are large language models really good at generating complex structured data?'' 2024.

\bibitem{xu2024emerging}
X.~Xu, Y.~Wu, P.~Liang, Y.~He, and H.~Wang, ``Emerging synergies between large language models and machine learning in ecommerce recommendations,'' 2024.

\bibitem{zhang2021ebert}
D.~Zhang, Z.~Yuan, Y.~Liu, F.~Zhuang, H.~Chen, and H.~Xiong, ``E-bert: A phrase and product knowledge enhanced language model for e-commerce,'' 2021.

\bibitem{zhou2023leveraging}
J.~Zhou, B.~Liu, J.~N. A.~Y. Hong, K.~chih Lee, and M.~Wen, ``Leveraging large language models for enhanced product descriptions in ecommerce,'' 2023.

\bibitem{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster, D.~Simig, P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer, ``Opt: Open pre-trained transformer language models,'' 2022.

\bibitem{yuan2023scaling}
Z.~Yuan, H.~Yuan, C.~Li, G.~Dong, K.~Lu, C.~Tan, C.~Zhou, and J.~Zhou, ``Scaling relationship on learning mathematical reasoning with large language models,'' 2023.

\bibitem{creswell2022selectioninference}
A.~Creswell, M.~Shanahan, and I.~Higgins, ``Selection-inference: Exploiting large language models for interpretable logical reasoning,'' 2022.

\bibitem{zhou-etal-2023-inform}
\BIBentryALTinterwordspacing
C.~Zhou, W.~You, J.~Li, J.~Ye, K.~Chen, and M.~Zhang, ``{INFORM} : Information e{N}tropy based multi-step reasoning {FOR} large language models,'' in \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, H.~Bouamor, J.~Pino, and K.~Bali, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Singapore: Association for Computational Linguistics, Dec. 2023, pp. 3565--3576. [Online]. Available: \url{https://aclanthology.org/2023.emnlp-main.216}
\BIBentrySTDinterwordspacing

\bibitem{wang2024largescale}
X.~Wang, G.~Chen, G.~Qian, P.~Gao, X.-Y. Wei, Y.~Wang, Y.~Tian, and W.~Gao, ``Large-scale multi-modal pre-trained models: A comprehensive survey,'' 2024.

\bibitem{minaee2024large}
S.~Minaee, T.~Mikolov, N.~Nikzad, M.~Chenaghlu, R.~Socher, X.~Amatriain, and J.~Gao, ``Large language models: A survey,'' 2024.

\bibitem{OnePlusNord35G2023}
\BIBentryALTinterwordspacing
Pricebaba.com, ``Oneplus nord 3 5g - specifications and reviews,'' 2023, accessed: 2023-07-13. [Online]. Available: \url{https://pricebaba.com/mobile/oneplus-nord-3-5g}
\BIBentrySTDinterwordspacing

\bibitem{zhao2023qtsummqueryfocusedsummarizationtabular}
\BIBentryALTinterwordspacing
Y.~Zhao, Z.~Qi, L.~Nan, B.~Mi, Y.~Liu, W.~Zou, S.~Han, R.~Chen, X.~Tang, Y.~Xu, D.~Radev, and A.~Cohan, ``Qtsumm: Query-focused summarization over tabular data,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2305.14303}
\BIBentrySTDinterwordspacing

\end{thebibliography}
