% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{he2023survey}
K.~He, R.~Mao, Q.~Lin, Y.~Ruan, X.~Lan, M.~Feng, and E.~Cambria, ``A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics,'' 2023.

\bibitem{REDDY2023101304}
\BIBentryALTinterwordspacing
S.~Reddy, ``Evaluating large language models for use in healthcare: A framework for translational value assessment,'' \emph{Informatics in Medicine Unlocked}, vol.~41, p. 101304, 2023. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S2352914823001508}
\BIBentrySTDinterwordspacing

\bibitem{Varshney_2024}
\BIBentryALTinterwordspacing
T.~Varshney, ``Build an llm-powered data agent for data analysis,'' Feb 2024. [Online]. Available: \url{https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/}
\BIBentrySTDinterwordspacing

\bibitem{Bergmann_2024}
\BIBentryALTinterwordspacing
D.~Bergmann, ``Build an llm-powered data agent for data analysis,'' March 2024. [Online]. Available: \url{https://www.ibm.com/topics/fine-tuning}
\BIBentrySTDinterwordspacing

\bibitem{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C. Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom, ``Llama 2: Open foundation and fine-tuned chat models,'' 2023.

\bibitem{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~de~las Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R. Lavaud, M.-A. Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and W.~E. Sayed, ``Mistral 7b,'' 2023.

\bibitem{zhuang2024structlm}
A.~Zhuang, G.~Zhang, T.~Zheng, X.~Du, J.~Wang, W.~Ren, S.~W. Huang, J.~Fu, X.~Yue, and W.~Chen, ``Structlm: Towards building generalist models for structured knowledge grounding,'' 2024.

\bibitem{singha2023tabular}
A.~Singha, J.~Cambronero, S.~Gulwani, V.~Le, and C.~Parnin, ``Tabular representation, noisy operators, and impacts on table structure understanding tasks in llms,'' 2023.

\bibitem{gao2024jsontuning}
C.~Gao, W.~Zhang, G.~Chen, and W.~Lam, ``Jsontuning: Towards generalizable, robust, and controllable instruction tuning,'' 2024.

\bibitem{macková2023promap}
K.~Macková and M.~Pilát, ``Promap: Datasets for product mapping in e-commerce,'' 2023.

\bibitem{zhao2023survey}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang, J.~Zhang, Z.~Dong, Y.~Du, C.~Yang, Y.~Chen, Z.~Chen, J.~Jiang, R.~Ren, Y.~Li, X.~Tang, Z.~Liu, P.~Liu, J.-Y. Nie, and J.-R. Wen, ``A survey of large language models,'' 2023.

\bibitem{naveed2024comprehensive}
H.~Naveed, A.~U. Khan, S.~Qiu, M.~Saqib, S.~Anwar, M.~Usman, N.~Akhtar, N.~Barnes, and A.~Mian, ``A comprehensive overview of large language models,'' 2024.

\bibitem{10305960}
M.~Debbah, ``Large language models for telecom,'' in \emph{2023 Eighth International Conference on Fog and Mobile Edge Computing (FMEC)}, 2023, pp. 3--4.

\bibitem{fan2023fatellm}
T.~Fan, Y.~Kang, G.~Ma, W.~Chen, W.~Wei, L.~Fan, and Q.~Yang, ``Fate-llm: A industrial grade federated learning framework for large language models,'' 2023.

\bibitem{xu-etal-2021-raise}
\BIBentryALTinterwordspacing
R.~Xu, F.~Luo, Z.~Zhang, C.~Tan, B.~Chang, S.~Huang, and F.~Huang, ``Raise a child in large language model: Towards effective and generalizable fine-tuning,'' in \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, M.-F. Moens, X.~Huang, L.~Specia, and S.~W.-t. Yih, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 9514--9528. [Online]. Available: \url{https://aclanthology.org/2021.emnlp-main.749}
\BIBentrySTDinterwordspacing

\bibitem{sun2023comparative}
X.~Sun, Y.~Ji, B.~Ma, and X.~Li, ``A comparative study between full-parameter and lora-based fine-tuning on chinese instruction data for instruction following large language model,'' 2023.

\bibitem{yu2022differentially}
D.~Yu, S.~Naik, A.~Backurs, S.~Gopi, H.~A. Inan, G.~Kamath, J.~Kulkarni, Y.~T. Lee, A.~Manoel, L.~Wutschitz, S.~Yekhanin, and H.~Zhang, ``Differentially private fine-tuning of language models,'' 2022.

\bibitem{zheng2024llamafactory}
Y.~Zheng, R.~Zhang, J.~Zhang, Y.~Ye, Z.~Luo, and Y.~Ma, ``Llamafactory: Unified efficient fine-tuning of 100+ language models,'' 2024.

\bibitem{zhu2024lift}
\BIBentryALTinterwordspacing
L.~Zhu, L.~Hu, J.~Lin, and S.~Han, ``{LIFT}: Efficient layer-wise fine-tuning for large model models,'' 2024. [Online]. Available: \url{https://openreview.net/forum?id=u0INlprg3U}
\BIBentrySTDinterwordspacing

\bibitem{Muntjir2016}
\BIBentryALTinterwordspacing
M.~Muntjir and A.~T. Siddiqui, ``An enhanced framework with advanced study to incorporate the searching of e-commerce products using modernization of database queries,'' \emph{International Journal of Advanced Computer Science and Applications}, vol.~7, no.~5, 2016. [Online]. Available: \url{http://dx.doi.org/10.14569/IJACSA.2016.070514}
\BIBentrySTDinterwordspacing

\bibitem{Liang_2020}
\BIBentryALTinterwordspacing
J.-H. Liang, ``Application of big data technology in product selection on cross-border e-commerce platforms,'' \emph{Journal of Physics: Conference Series}, vol. 1601, no.~3, p. 032012, jul 2020. [Online]. Available: \url{https://dx.doi.org/10.1088/1742-6596/1601/3/032012}
\BIBentrySTDinterwordspacing

\bibitem{10.1007/978-3-319-20895-4_34}
W.-K. Tan and H.-H. Teo, ``Productpedia -- a collaborative electronic product catalog for ecommerce 3.0,'' in \emph{HCI in Business}, F.~Fui-Hoon~Nah and C.-H. Tan, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Cham: Springer International Publishing, 2015, pp. 370--381.

\bibitem{10.1145/3583780.3615503}
\BIBentryALTinterwordspacing
G.~Ryali, S.~S, S.~Kaveri, and P.~M. Comar, ``Trendspotter: Forecasting e-commerce product trends,'' in \emph{Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}, ser. CIKM '23.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2023, p. 4808–4814. [Online]. Available: \url{https://doi.org/10.1145/3583780.3615503}
\BIBentrySTDinterwordspacing

\bibitem{chen-etal-2022-bert2bert}
\BIBentryALTinterwordspacing
C.~Chen, Y.~Yin, L.~Shang, X.~Jiang, Y.~Qin, F.~Wang, Z.~Wang, X.~Chen, Z.~Liu, and Q.~Liu, ``bert2{BERT}: Towards reusable pretrained language models,'' in \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, S.~Muresan, P.~Nakov, and A.~Villavicencio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 2134--2148. [Online]. Available: \url{https://aclanthology.org/2022.acl-long.151}
\BIBentrySTDinterwordspacing

\bibitem{agrawal2022large}
M.~Agrawal, S.~Hegselmann, H.~Lang, Y.~Kim, and D.~Sontag, ``Large language models are few-shot clinical information extractors,'' 2022.

\bibitem{edunov-etal-2019-pre}
\BIBentryALTinterwordspacing
S.~Edunov, A.~Baevski, and M.~Auli, ``Pre-trained language model representations for language generation,'' in \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4052--4059. [Online]. Available: \url{https://aclanthology.org/N19-1409}
\BIBentrySTDinterwordspacing

\bibitem{brinkmann2024product}
A.~Brinkmann, R.~Shraga, and C.~Bizer, ``Product attribute value extraction using large language models,'' 2024.

\bibitem{labrak2024biomistral}
Y.~Labrak, A.~Bazoge, E.~Morin, P.-A. Gourraud, M.~Rouvier, and R.~Dufour, ``Biomistral: A collection of open-source pretrained large language models for medical domains,'' 2024.

\bibitem{skondras2023generating}
P.~Skondras, P.~Zervas, and G.~Tzimas, ``Generating synthetic resume data with large language models for enhanced job description classification,'' \emph{Future Internet}, vol.~15, no.~11, p. 363, 2023.

\bibitem{tang2024strucbench}
X.~Tang, Y.~Zong, J.~Phang, Y.~Zhao, W.~Zhou, A.~Cohan, and M.~Gerstein, ``Struc-bench: Are large language models really good at generating complex structured data?'' 2024.

\bibitem{xu2024emerging}
X.~Xu, Y.~Wu, P.~Liang, Y.~He, and H.~Wang, ``Emerging synergies between large language models and machine learning in ecommerce recommendations,'' 2024.

\bibitem{zhang2021ebert}
D.~Zhang, Z.~Yuan, Y.~Liu, F.~Zhuang, H.~Chen, and H.~Xiong, ``E-bert: A phrase and product knowledge enhanced language model for e-commerce,'' 2021.

\bibitem{zhou2023leveraging}
J.~Zhou, B.~Liu, J.~N. A.~Y. Hong, K.~chih Lee, and M.~Wen, ``Leveraging large language models for enhanced product descriptions in ecommerce,'' 2023.

\bibitem{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster, D.~Simig, P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer, ``Opt: Open pre-trained transformer language models,'' 2022.

\bibitem{yuan2023scaling}
Z.~Yuan, H.~Yuan, C.~Li, G.~Dong, K.~Lu, C.~Tan, C.~Zhou, and J.~Zhou, ``Scaling relationship on learning mathematical reasoning with large language models,'' 2023.

\bibitem{creswell2022selectioninference}
A.~Creswell, M.~Shanahan, and I.~Higgins, ``Selection-inference: Exploiting large language models for interpretable logical reasoning,'' 2022.

\bibitem{zhou-etal-2023-inform}
\BIBentryALTinterwordspacing
C.~Zhou, W.~You, J.~Li, J.~Ye, K.~Chen, and M.~Zhang, ``{INFORM} : Information e{N}tropy based multi-step reasoning {FOR} large language models,'' in \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, H.~Bouamor, J.~Pino, and K.~Bali, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Singapore: Association for Computational Linguistics, Dec. 2023, pp. 3565--3576. [Online]. Available: \url{https://aclanthology.org/2023.emnlp-main.216}
\BIBentrySTDinterwordspacing

\bibitem{wang2024largescale}
X.~Wang, G.~Chen, G.~Qian, P.~Gao, X.-Y. Wei, Y.~Wang, Y.~Tian, and W.~Gao, ``Large-scale multi-modal pre-trained models: A comprehensive survey,'' 2024.

\bibitem{minaee2024large}
S.~Minaee, T.~Mikolov, N.~Nikzad, M.~Chenaghlu, R.~Socher, X.~Amatriain, and J.~Gao, ``Large language models: A survey,'' 2024.

\end{thebibliography}
