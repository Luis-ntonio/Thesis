% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{he2023survey}
K.~He, R.~Mao, Q.~Lin, Y.~Ruan, X.~Lan, M.~Feng, and E.~Cambria, ``A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics,'' 2023.

\bibitem{REDDY2023101304}
\BIBentryALTinterwordspacing
S.~Reddy, ``Evaluating large language models for use in healthcare: A framework for translational value assessment,'' \emph{Informatics in Medicine Unlocked}, vol.~41, p. 101304, 2023. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S2352914823001508}
\BIBentrySTDinterwordspacing

\bibitem{Varshney_2024}
\BIBentryALTinterwordspacing
T.~Varshney, ``Build an llm-powered data agent for data analysis,'' Feb 2024. [Online]. Available: \url{https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/}
\BIBentrySTDinterwordspacing

\bibitem{Bergmann_2024}
\BIBentryALTinterwordspacing
D.~Bergmann, ``Build an llm-powered data agent for data analysis,'' March 2024. [Online]. Available: \url{https://www.ibm.com/topics/fine-tuning}
\BIBentrySTDinterwordspacing

\bibitem{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C. Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom, ``Llama 2: Open foundation and fine-tuned chat models,'' 2023.

\bibitem{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~de~las Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R. Lavaud, M.-A. Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and W.~E. Sayed, ``Mistral 7b,'' 2023.

\bibitem{zhuang2024structlm}
A.~Zhuang, G.~Zhang, T.~Zheng, X.~Du, J.~Wang, W.~Ren, S.~W. Huang, J.~Fu, X.~Yue, and W.~Chen, ``Structlm: Towards building generalist models for structured knowledge grounding,'' 2024.

\bibitem{singha2023tabular}
A.~Singha, J.~Cambronero, S.~Gulwani, V.~Le, and C.~Parnin, ``Tabular representation, noisy operators, and impacts on table structure understanding tasks in llms,'' 2023.

\bibitem{gao2024jsontuning}
C.~Gao, W.~Zhang, G.~Chen, and W.~Lam, ``Jsontuning: Towards generalizable, robust, and controllable instruction tuning,'' 2024.

\bibitem{macková2023promap}
K.~Macková and M.~Pilát, ``Promap: Datasets for product mapping in e-commerce,'' 2023.

\bibitem{zhao2023survey}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang, J.~Zhang, Z.~Dong, Y.~Du, C.~Yang, Y.~Chen, Z.~Chen, J.~Jiang, R.~Ren, Y.~Li, X.~Tang, Z.~Liu, P.~Liu, J.-Y. Nie, and J.-R. Wen, ``A survey of large language models,'' 2023.

\bibitem{naveed2024comprehensive}
H.~Naveed, A.~U. Khan, S.~Qiu, M.~Saqib, S.~Anwar, M.~Usman, N.~Akhtar, N.~Barnes, and A.~Mian, ``A comprehensive overview of large language models,'' 2024.

\bibitem{10305960}
M.~Debbah, ``Large language models for telecom,'' in \emph{2023 Eighth International Conference on Fog and Mobile Edge Computing (FMEC)}, 2023, pp. 3--4.

\bibitem{fan2023fatellm}
T.~Fan, Y.~Kang, G.~Ma, W.~Chen, W.~Wei, L.~Fan, and Q.~Yang, ``Fate-llm: A industrial grade federated learning framework for large language models,'' 2023.

\bibitem{xu-etal-2021-raise}
\BIBentryALTinterwordspacing
R.~Xu, F.~Luo, Z.~Zhang, C.~Tan, B.~Chang, S.~Huang, and F.~Huang, ``Raise a child in large language model: Towards effective and generalizable fine-tuning,'' in \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, M.-F. Moens, X.~Huang, L.~Specia, and S.~W.-t. Yih, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 9514--9528. [Online]. Available: \url{https://aclanthology.org/2021.emnlp-main.749}
\BIBentrySTDinterwordspacing

\bibitem{sun2023comparative}
X.~Sun, Y.~Ji, B.~Ma, and X.~Li, ``A comparative study between full-parameter and lora-based fine-tuning on chinese instruction data for instruction following large language model,'' 2023.

\bibitem{yu2022differentially}
D.~Yu, S.~Naik, A.~Backurs, S.~Gopi, H.~A. Inan, G.~Kamath, J.~Kulkarni, Y.~T. Lee, A.~Manoel, L.~Wutschitz, S.~Yekhanin, and H.~Zhang, ``Differentially private fine-tuning of language models,'' 2022.

\bibitem{zheng2024llamafactory}
Y.~Zheng, R.~Zhang, J.~Zhang, Y.~Ye, Z.~Luo, and Y.~Ma, ``Llamafactory: Unified efficient fine-tuning of 100+ language models,'' 2024.

\bibitem{zhu2024lift}
\BIBentryALTinterwordspacing
L.~Zhu, L.~Hu, J.~Lin, and S.~Han, ``{LIFT}: Efficient layer-wise fine-tuning for large model models,'' 2024. [Online]. Available: \url{https://openreview.net/forum?id=u0INlprg3U}
\BIBentrySTDinterwordspacing

\end{thebibliography}
